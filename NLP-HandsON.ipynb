{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Gensim.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "ZPGaNQtGj3pM",
        "5X_Xqu4V90w4",
        "4QaCKno-qh4f",
        "cdK63eRMww9D",
        "pRLzfgCuM0Ts"
      ],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/sourav044/Python/blob/master/NLP-HandsON.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wUTaLTnazoAp",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from gensim.test.utils import common_texts\n",
        "from gensim.models import Word2Vec\n",
        "from gensim.similarities import WmdSimilarity\n",
        "text = ['human', 'interface', 'computer']\n",
        "model = Word2Vec(common_texts, size=20, min_count=1)  # train word-vectors\n",
        "\n",
        "index = WmdSimilarity(text, model)\n",
        "# Make query.\n",
        "query = ['trees']\n",
        "sims = index[query]\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Z2-cxm0e1nlr",
        "colab_type": "code",
        "outputId": "ac511af3-61f3-483c-f30d-b3413ffa2b5f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 89
        }
      },
      "source": [
        "print(common_texts)\n",
        "print(model)\n",
        "print(sims)"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[['human', 'interface', 'computer'], ['survey', 'user', 'computer', 'system', 'response', 'time'], ['eps', 'user', 'interface', 'system'], ['system', 'human', 'system', 'eps'], ['user', 'response', 'time'], ['trees'], ['graph', 'trees'], ['graph', 'minors', 'trees'], ['graph', 'minors', 'survey']]\n",
            "Word2Vec(vocab=12, size=20, alpha=0.025)\n",
            "[0. 0. 0.]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oXNlVMY431pE",
        "colab_type": "code",
        "outputId": "1fad8440-60c6-4f40-828b-9dde2b64cfce",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 89
        }
      },
      "source": [
        "from gensim.test.utils import common_texts\n",
        "from gensim.models import Word2Vec\n",
        "from gensim.similarities import WmdSimilarity\n",
        "\n",
        "model = Word2Vec(common_texts, size=20, min_count=1)  # train word-vectors\n",
        "index = WmdSimilarity(common_texts, model)\n",
        " # Make query.\n",
        "query = ['human', 'interface', 'computer']\n",
        "sims = index[query]\n",
        "print(common_texts)\n",
        "print(sims)"
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[['human', 'interface', 'computer'], ['survey', 'user', 'computer', 'system', 'response', 'time'], ['eps', 'user', 'interface', 'system'], ['system', 'human', 'system', 'eps'], ['user', 'response', 'time'], ['trees'], ['graph', 'trees'], ['graph', 'minors', 'trees'], ['graph', 'minors', 'survey']]\n",
            "[1.         0.48811893 0.49534619 0.47371859 0.4654382  0.41035556\n",
            " 0.42039188 0.43218044 0.42965077]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "k87koUW49mKc",
        "colab_type": "text"
      },
      "source": [
        "NOW starting the debug "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nIdwOjiv-FL6",
        "colab_type": "code",
        "outputId": "d282f8d4-2fe5-4776-8500-a4aee6b4e8b9",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "pip install PyPDF2"
      ],
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: PyPDF2 in /usr/local/lib/python3.6/dist-packages (1.26.0)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Vb41P3B0kPst",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import nltk\n",
        "from nltk.tokenize import sent_tokenize"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YH_ToGs4-ROx",
        "colab_type": "code",
        "outputId": "3a918987-afed-480c-899b-33a3862c5274",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 69
        }
      },
      "source": [
        "nltk.download('stopwords') "
      ],
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 24
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nYZOlK3U-32D",
        "colab_type": "code",
        "outputId": "02002da7-73a2-4fc4-87a5-2df852a5051c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        }
      },
      "source": [
        "import spacy.cli\n",
        "lang = spacy.cli.download(\"en_core_web_sm\")\n",
        "nlp = spacy.load(\"en_core_web_sm\")\n"
      ],
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
            "You can now load the model via spacy.load('en_core_web_sm')\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TXLCNEZ49ygl",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from collections import Counter\n",
        "import nltk\n",
        "from nltk.tokenize import sent_tokenize\n",
        "from keras.preprocessing.text import Tokenizer \n",
        "import spacy\n",
        "import PyPDF2\n",
        "import keras\n",
        "import tensorflow as tf\n",
        "import gensim\n",
        "from nltk.corpus import stopwords"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-FVkZLUL7xmz",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "def combine(sub_keys, keywords_splits, lb, mb, ub):\n",
        "    whitespace = ' '\n",
        "    while mb != ub:\n",
        "        keywords_splits.append(whitespace.join(sub_keys[lb: mb]))\n",
        "        keywords_splits.append(whitespace.join(sub_keys[mb: ub]))\n",
        "        mb += 1\n",
        "    del sub_keys[0]\n",
        "    if len(sub_keys) > 2:\n",
        "        combine(sub_keys, keywords_splits, 0, 1, len(sub_keys))\n",
        "\n",
        "\n",
        "def keywords_splitter(keywords, keywords_splits):\n",
        "\n",
        "    for key in keywords:\n",
        "        sub_keys = key.split()\n",
        "\n",
        "        if len(sub_keys) > 2:\n",
        "            combine(sub_keys, keywords_splits, 0, 1, len(sub_keys))\n",
        "\n",
        "\n",
        "def pre_query(question_query):\n",
        "\n",
        "    keywords = question_query.get_features()\n",
        "\n",
        "    keywords = [feat.lower() for feat in keywords]\n",
        "    whitespace = ' '\n",
        "    keywords_splits = whitespace.join(keywords).split()\n",
        "\n",
        "    keywords_splitter(keywords, keywords_splits)\n",
        "    keywords_splits = list(set(keywords_splits + keywords))\n",
        "\n",
        "    return keywords_splits\n",
        "\n",
        "\n",
        "def query2vec(query, dictionary):\n",
        "\n",
        "    print(\"Searching: {0}\".format(query))\n",
        "    corpus = dictionary.doc2bow(query)\n",
        "\n",
        "    return corpus\n",
        "\n",
        "\n",
        "def doc2vec(documents):\n",
        "         \n",
        "    texts = [[word for word in doc.lemma_.split() if word not in ['a','b']]for doc in documents]\n",
        "\n",
        "    frequency = Counter()\n",
        "    for sent in texts:\n",
        "        for token in sent:\n",
        "            frequency[token] += 1\n",
        "\n",
        "    \n",
        "    text_token = [[token for token in snipp]for snipp in texts]\n",
        "    \n",
        "   \n",
        "    texts = [[token for token in snipp if frequency[token] > -1]for snipp in texts]\n",
        "\n",
        "    dictionary = gensim.corpora.Dictionary(texts)\n",
        "    corpus = [dictionary.doc2bow(snipp) for snipp in texts]\n",
        "\n",
        "    return corpus, dictionary, texts, text_token\n",
        "\n",
        "\n",
        "def transform_vec(corpus, query_corpus):\n",
        "    lsidf = gensim.models.LsiModel(corpus)\n",
        "\n",
        "    corpus_lsidf = lsidf[corpus]\n",
        "    query_lsidf = lsidf[query_corpus]\n",
        "\n",
        "    return corpus_lsidf, query_lsidf\n",
        "\n",
        "\n",
        "def similarity(corpus_lsidf, query_lsidf):\n",
        "    index = gensim.similarities.SparseMatrixSimilarity(corpus_lsidf, num_features=200000)\n",
        "\n",
        "    simi = index[query_lsidf]\n",
        "\n",
        "    simi_sorted = sorted(enumerate(simi), key=lambda item: -item[1])\n",
        "    return simi_sorted\n",
        "\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Z7v077Qu7yZ_",
        "colab_type": "code",
        "outputId": "eac3c94d-f3ee-44c2-f4dc-91bca5ab7353",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 55
        }
      },
      "source": [
        "text = \"My name is Sourav and I am from India. My age is 24. I am currently stying in Passau. I am studing Mobile and embedded system. My father name is Uttam Kumar gupta. He is a business man. He own a shop. His work is of Photography. \"\n",
        "en_doc = nlp(u'' + text)\n",
        "sentences = list(en_doc.sents)\n",
        "print(sentences)\n"
      ],
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[My name is Sourav and I am from India., My age is 24., I am currently stying in Passau., I am studing Mobile and embedded system., My father name is Uttam Kumar gupta., He is a business man., He own a shop., His work is of Photography.]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZPGaNQtGj3pM",
        "colab_type": "text"
      },
      "source": [
        "### **1ST Remove Stop Word and Bag of word**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_bBCdfNF9q0b",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "corpus, dictionary, text, token = doc2vec(sentences)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hL0Cj4wSFss-",
        "colab_type": "code",
        "outputId": "e725ace3-9990-4a1f-86ef-0de8a19eef89",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 89
        }
      },
      "source": [
        "print(text)\n",
        "print(token)\n",
        "print(dictionary)"
      ],
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[['-PRON-', 'name', 'be', 'Sourav', 'and', '-PRON-', 'be', 'from', 'India', '.'], ['-PRON-', 'age', 'be', '24', '.'], ['-PRON-', 'be', 'currently', 'stye', 'in', 'Passau', '.'], ['-PRON-', 'be', 'stud', 'Mobile', 'and', 'embedded', 'system', '.'], ['-PRON-', 'father', 'name', 'be', 'Uttam', 'Kumar', 'gupta', '.'], ['-PRON-', 'be', 'business', 'man', '.'], ['-PRON-', 'own', 'shop', '.'], ['-PRON-', 'work', 'be', 'of', 'Photography', '.']]\n",
            "[['-PRON-', 'name', 'be', 'Sourav', 'and', '-PRON-', 'be', 'from', 'India', '.'], ['-PRON-', 'age', 'be', '24', '.'], ['-PRON-', 'be', 'currently', 'stye', 'in', 'Passau', '.'], ['-PRON-', 'be', 'stud', 'Mobile', 'and', 'embedded', 'system', '.'], ['-PRON-', 'father', 'name', 'be', 'Uttam', 'Kumar', 'gupta', '.'], ['-PRON-', 'be', 'business', 'man', '.'], ['-PRON-', 'own', 'shop', '.'], ['-PRON-', 'work', 'be', 'of', 'Photography', '.']]\n",
            "Dictionary(29 unique tokens: ['-PRON-', '.', 'India', 'Sourav', 'and']...)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "a_S3D4YoGOH8",
        "colab_type": "code",
        "outputId": "b1f6b799-7007-48da-ea35-18452f926b46",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 55
        }
      },
      "source": [
        "print(corpus)"
      ],
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[(0, 2), (1, 1), (2, 1), (3, 1), (4, 1), (5, 2), (6, 1), (7, 1)], [(0, 1), (1, 1), (5, 1), (8, 1), (9, 1)], [(0, 1), (1, 1), (5, 1), (10, 1), (11, 1), (12, 1), (13, 1)], [(0, 1), (1, 1), (4, 1), (5, 1), (14, 1), (15, 1), (16, 1), (17, 1)], [(0, 1), (1, 1), (5, 1), (7, 1), (18, 1), (19, 1), (20, 1), (21, 1)], [(0, 1), (1, 1), (5, 1), (22, 1), (23, 1)], [(0, 1), (1, 1), (24, 1), (25, 1)], [(0, 1), (1, 1), (5, 1), (26, 1), (27, 1), (28, 1)]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4QaCKno-qh4f",
        "colab_type": "text"
      },
      "source": [
        "### **Feature Extract**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9jLJzn2SyjKh",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "def get_detail(sentence):\n",
        "    for token in sentence:\n",
        "        print(\"{0} -- Lemma:{1}, Tag:{2}, EntType:{3}, Dep:{4}, Head:{5}\"\n",
        "                     .format(token.text, token.lemma_, token.tag_, token.ent_type_, token.dep_, token.head))\n",
        "\n",
        "\n",
        "def get_compound_nouns(en_doc, token, token_text):\n",
        "\n",
        "    \"\"\"\n",
        "    Recursively find the left and right compound nouns\n",
        "    \"\"\"\n",
        "\n",
        "    parent_token = token\n",
        "\n",
        "    print(\"Compound Noun:{0} DEP {1}\".format(token.text, token.dep_))\n",
        "\n",
        "    # If previous token is a compound noun\n",
        "    while token.i > 0 and en_doc[token.i - 1].dep_ == \"compound\":\n",
        "        token_text = en_doc[token.i - 1].text + \" \" + token_text\n",
        "        token = en_doc[token.i - 1]\n",
        "        # if the compound noun has any adjective modifier\n",
        "        token_text = get_adj_phrase(token, token_text)\n",
        "\n",
        "    token = parent_token\n",
        "\n",
        "    # If next token is a compound noun\n",
        "    while token.i < len(en_doc) - 1 and en_doc[token.i + 1].dep_ == \"compound\":\n",
        "        token_text = token_text + \" \" + en_doc[token.i + 1].text\n",
        "        token = en_doc[token.i + 1]\n",
        "        # if the compound noun has any adjective modifier\n",
        "        token_text = get_adj_phrase(token, token_text)\n",
        "\n",
        "    # NOTE: Can token.shape_ == Xxxx... or XXXX... token.ent_iob_ help us here ...?\n",
        "\n",
        "    return token_text\n",
        "\n",
        "def this_is_adjective(dep_):\n",
        "    return dep_ == \"amod\" or dep_ == \"acomp\" or dep_ == \"ccomp\"\n",
        "\n",
        "def get_adj_phrase(token, token_text):\n",
        "\n",
        "    \"\"\"\n",
        "    To fetch all the adjectives describing the noun\n",
        "    \"\"\"\n",
        "\n",
        "    # amod: An adjectival modifier of a noun is any adjectival phrase that serves to modify the meaning of the noun.\n",
        "    # ccomp: A clausal complement of a verb or adjective is a dependent clause which is a core argument.\n",
        "    #        That is, it functions like an object of the verb, or adjective.\n",
        "    # acomp: An adjectival complement of a verb is an adjectival phrase which functions as the complement\n",
        "\n",
        "    for child in token.children:\n",
        "        if(this_is_adjective(child.dep_)) and child.text != \"much\" and child.text != \"many\":\n",
        "            # if child.dep_ == \"amod\" or child.dep_ == \"acomp\" or child.dep_ == \"ccomp\":  # not for how many\n",
        "            #     if child.text != \"much\" and child.text != \"many\":\n",
        "            token_text = child.lemma_ + \" \" + token_text\n",
        "    return token_text\n",
        "\n",
        "\n",
        "def get_root_phrase(token, keywords):\n",
        "\n",
        "    # xcomp: An open clausal complement (xcomp) of a verb or an adjective is a predicative or clausal complement\n",
        "    #        without its own subject.\n",
        "\n",
        "    for child in token.children:\n",
        "        if child.dep_ == \"acomp\" or child.dep_ == \"xcomp\" or child.dep_ == \"ccomp\":\n",
        "            keywords.append(child.lemma_)\n",
        "    return keywords\n",
        "\n",
        "\n",
        "def this_is_noun(tag_):\n",
        "    # If is Noun/Proper Noun, be it Singular or Plural\n",
        "    return tag_ == \"NN\" or tag_ == \"NNP\" or tag_ == \"NNPS\" or tag_ == \"NNS\"\n",
        "\n",
        "\n",
        "def get_noun_chunk(sentence, en_doc, keywords):\n",
        "\n",
        "    root_word = \"\"\n",
        "\n",
        "    for token in sentence:\n",
        "\n",
        "        # If the Noun itself is not a compound Noun then we can find its compound Nouns\n",
        "        print(token)\n",
        "        print(\"+sentence\")\n",
        "        if(this_is_noun(token.tag_)) and token.dep_ != \"compound\":\n",
        "            token_text = get_compound_nouns(en_doc, token, token.text)\n",
        "            keywords.append(token_text)\n",
        "\n",
        "        if token.tag_ == \"JJ\" and token.dep_ == \"attr\":\n",
        "            token_text = get_compound_nouns(en_doc, token, token.text)\n",
        "            token_text = get_adj_phrase(token, token_text)\n",
        "            keywords.append(token_text)\n",
        "\n",
        "        # If is a Cardinal Number & dependency is numeric modifier\n",
        "        # nummod : A numeric modifier of a noun is any number phrase that\n",
        "        #          serves to modify the meaning of the noun with a quantity.\n",
        "        if token.dep_ == \"nummod\" or token.tag_ == \"CD\":\n",
        "            token_text = token.text\n",
        "\n",
        "            if token.i > 0 and en_doc[token.i - 1].tag_ == \"JJ\":\n",
        "                # If previous token is Adjective, the adjective is liked with the cardinal number\n",
        "                token_text = en_doc[token.i - 1].text + \" \" + token.text\n",
        "\n",
        "            if token.i < len(en_doc) - 1 and en_doc[token.i + 1].tag_ == \"JJ\":\n",
        "                # If next token is Adjective\n",
        "                token_text = token.text + \" \" + en_doc[token.i + 1].text\n",
        "\n",
        "            keywords.append(token_text)\n",
        "\n",
        "        # Extracts the root word of sentence\n",
        "        if token.dep_ == \"ROOT\":\n",
        "            root_word = token.lemma_\n",
        "            print(\"++sentence\")\n",
        "            print(token)\n",
        "            print(\"++sentence\")\n",
        "            keywords = get_root_phrase(token, keywords)\n",
        "\n",
        "    return root_word, keywords\n",
        "\n",
        "\n",
        "def extract_features(question_type, en_doc, show_debug=False):\n",
        "\n",
        "    # NOTE: In the whole pipeline question_type is not used anywhere currently...\n",
        "\n",
        "    keywords = []\n",
        "\n",
        "    for sentence in en_doc.sents:\n",
        "      print(sentence)\n",
        "      print(\"+sentence\")\n",
        "      if show_debug:\n",
        "        get_detail(sentence)\n",
        "      root, keywords = get_noun_chunk(sentence, en_doc, keywords)\n",
        "      keywords.append(root)\n",
        "\n",
        "    return keywords\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jmuNpBnMylp2",
        "colab_type": "code",
        "outputId": "252b7c61-bc92-460b-8e59-8bcda3f2cb6f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 468
        }
      },
      "source": [
        "question = \"What is age of sourav?\"\n",
        "en_doc_l = nlp(u'' + question)\n",
        "print(\"Extracted: {0}\".format(extract_features(\"\", en_doc_l, True)))"
      ],
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "What is age of sourav?\n",
            "+sentence\n",
            "What -- Lemma:what, Tag:WP, EntType:, Dep:nsubj, Head:is\n",
            "is -- Lemma:be, Tag:VBZ, EntType:, Dep:ROOT, Head:is\n",
            "age -- Lemma:age, Tag:NN, EntType:, Dep:nsubj, Head:is\n",
            "of -- Lemma:of, Tag:IN, EntType:, Dep:prep, Head:age\n",
            "sourav -- Lemma:sourav, Tag:NNS, EntType:, Dep:pobj, Head:of\n",
            "? -- Lemma:?, Tag:., EntType:, Dep:punct, Head:is\n",
            "What\n",
            "+sentence\n",
            "is\n",
            "+sentence\n",
            "++sentence\n",
            "is\n",
            "++sentence\n",
            "age\n",
            "+sentence\n",
            "Compound Noun:age DEP nsubj\n",
            "of\n",
            "+sentence\n",
            "sourav\n",
            "+sentence\n",
            "Compound Noun:sourav DEP pobj\n",
            "?\n",
            "+sentence\n",
            "Extracted: ['age', 'sourav', 'be']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "prQFIGv9btBc",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "cccd5b7d-1dbc-432e-f7e7-3690aa1af11c"
      },
      "source": [
        "doc = nlp('European authorities fined Google a record $5.1 billion on Wednesday for abusing its power in the mobile phone market and ordered the company to alter its practices')\n",
        "print([(X.text, X.label_) for X in doc.ents])"
      ],
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[('European', 'NORP'), ('Google', 'ORG'), ('$5.1 billion', 'MONEY'), ('Wednesday', 'DATE')]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "x_SG8RK5ktdb",
        "colab_type": "text"
      },
      "source": [
        "### **transform_vec**\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5-yYY9PTRdFH",
        "colab_type": "code",
        "outputId": "a0668d5d-0d0c-444d-c913-9a7be518f59f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 364
        }
      },
      "source": [
        "question = \"What is age of sourav kumar\"\n",
        "en_doc_l = nlp(u'' + question)\n",
        "features = extract_features(\"\", en_doc_l, False)\n",
        "print(features)"
      ],
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "What is age of sourav kumar\n",
            "+sentence\n",
            "What\n",
            "+sentence\n",
            "is\n",
            "+sentence\n",
            "++sentence\n",
            "is\n",
            "++sentence\n",
            "age\n",
            "+sentence\n",
            "Compound Noun:age DEP attr\n",
            "of\n",
            "+sentence\n",
            "sourav\n",
            "+sentence\n",
            "kumar\n",
            "+sentence\n",
            "Compound Noun:kumar DEP pobj\n",
            "['age', 'sourav kumar', 'be']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KO92BfeInGkG",
        "colab_type": "code",
        "outputId": "735dedc1-de88-490f-b25e-5b603345bb04",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 55
        }
      },
      "source": [
        "import re\n",
        "#print([word.split() for word in features])\n",
        "ques = [word.split() for word in features]\n",
        "\n",
        "mix_ques = ([[x,y] for x in ques for y in ques if x != y])\n",
        "\n",
        "#listToString = str(','.join([str(elem) for elem in mix_ques]) + \",\"+ question +\",\"+ ','.join([str(elem) for elem in ques])) \n",
        " \n",
        "print([' '.join([str(c) for c in lst]) for lst in mix_ques])\n"
      ],
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[\"['age'] ['sourav', 'kumar']\", \"['age'] ['be']\", \"['sourav', 'kumar'] ['age']\", \"['sourav', 'kumar'] ['be']\", \"['be'] ['age']\", \"['be'] ['sourav', 'kumar']\"]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wr9el-O_WpCv",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 55
        },
        "outputId": "de76c608-1223-44c3-99b3-7fd5d7f8281b"
      },
      "source": [
        "# input list \n",
        "l = mix_ques\n",
        " \n",
        "\n",
        "# output list \n",
        "def lsToString(s):  \n",
        "    \n",
        "    # initialize an empty string \n",
        "    str1 = \"\"  \n",
        "    \n",
        "    # traverse in the string   \n",
        "    for ele in s: \n",
        "      if type(ele) == list:\n",
        "        str1 += \" \" +lsToString(ele)\n",
        "      else: \n",
        "        str1 += \" \" +ele   \n",
        "    \n",
        "    # return string   \n",
        "    return str1 \n",
        "  \n",
        "# function used for removing nested  \n",
        "# lists in python.  \n",
        "def removNestings(l): \n",
        "    output = [] \n",
        "    for i in l: \n",
        "        if type(i) == list: \n",
        "           output.append(removNestings(i)) \n",
        "        else: \n",
        "            output.append(i) \n",
        "    return output\n",
        "  \n",
        "\n",
        "def keywordMaker(l): \n",
        "    result = []\n",
        "    for i in l:    \n",
        "      result.append(lsToString(removNestings(i)))      \n",
        "    return result\n",
        "  \n",
        "\n",
        "ques_Maker = keywordMaker(l)\n",
        "ques_Maker.append(question) \n",
        "print(ques_Maker + ques)\n",
        " \n"
      ],
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['  age  sourav kumar', '  age  be', '  sourav kumar  age', '  sourav kumar  be', '  be  age', '  be  sourav kumar', 'What is age of sourav kumar', ['age'], ['sourav', 'kumar'], ['be']]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LptD9PJdsKed",
        "colab_type": "text"
      },
      "source": [
        "### temp"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3lPeUMdsnLzL",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "query_corpus = query2vec([\"What is my age.\",\"age\",\"24\",\"name\"], dictionary)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Wpa0RWK0p9zj",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "print(corpus)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aheSOofDksr-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "corpus_lsidf, query_lsidf = transform_vec(corpus, query_corpus)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jGCjGnvwmq0h",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "print(corpus_lsidf)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ldP2jzxsqHKB",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "simi_sorted = similarity(corpus_lsidf, query_lsidf)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "d_Tzb5ycjshY",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "print(len(simi_sorted))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "guvm0R8x36b7",
        "colab": {}
      },
      "source": [
        "    \n",
        "    print(\"simi_sorted \\n\")\n",
        "    print(simi_sorted)\n",
        "    candidate_ans = []\n",
        "    \n",
        "    ##New Correction\n",
        "    for sent in simi_sorted:\n",
        "        sent_id = sent[0]\n",
        "        if sent[1] > 0:\n",
        "          candidate_ans.append(str(sentences[sent_id]))\n",
        "          print(candidate_ans)\n",
        "        \n",
        "        \n",
        "        "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "97j0tI1iHKP3",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        " from gensim.corpora import Dictionary\n",
        "dct = Dictionary([\"máma mele maso\".split(), \"ema má máma\".split()])\n",
        "ch = dct.doc2bow([\"this\", \"is\", \"máma\", \"máma\"])\n",
        "print(dct)\n",
        "print(ch)\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "U53e1ivZfvm4",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "dct.doc2bow([\"this\", \"is\", \"máma\"], return_missing=True)\n",
        "([(2, 1)], {u'this': 1, u'is': 1})"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5X_Xqu4V90w4",
        "colab_type": "text"
      },
      "source": [
        "### **dependency parse**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "G1nukUOE97eL",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "  print(candidate_ans)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NVzj7wyA-BLo",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "doc = nlp('My age is 24. My father name is Uttam Kumar gupta. My name is Sourav and I am from India.')\n",
        " \n",
        "for token in doc:\n",
        "    print(\"{0}/{1} <--{2}-- {3}/{4}\".format(\n",
        "        token.text, token.tag_, token.dep_, token.head.text, token.head.tag_))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3_bYtS05_Lga",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "spacy.explain(\"NNP\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZyEJpsVQkT39",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import spacy\n",
        "from spacy import displacy\n",
        " \n",
        "#displacy.serve(doc, style=\"dep\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uRpLlVa7FRf5",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import numpy as np\n",
        "import re\n",
        "from nltk import word_tokenize, pos_tag\n",
        "from nltk.corpus import stopwords\n",
        "from collections import Counter\n",
        "\n",
        "\n",
        "#input text file\n",
        "readFlNm=\"input.txt\"\n",
        "#precentage of summarization\n",
        "percentageOfSummary=70\n",
        "#fetch stop words\n",
        "stopWords = set(stopwords.words('english'))\n",
        "#update stop words\n",
        "stopWords.update(['\"', \"'\", ':', '(', ')', '[', ']', '{', '}']) #'.',  ',', '?', '!', ';'\n",
        "\n",
        "#using pos_tag get the tag of words\n",
        "def getTagsForWords(textLn2):\n",
        "    tokens=word_tokenize(textLn2)\n",
        "    tagged=pos_tag(tokens)\n",
        "    return(tagged)\n",
        "\n",
        "#remove stop words\n",
        "def remStopWordsOur(lineIn):\n",
        "    stopWords= {'i','a','and','about','an','are','as','at','be','by','com','for','from','how','in','is','it','not','of','on','or','that','the','this','to','was','what','when','where','who','will','with','the','www','your','is','am','some','you','your','I','A','And','About','An','Are','As','At','Be','By','Com','For','From','How','In','Is','It','Not','Of','On','Or','That','The','This','To','Was','What','When','Where','Who','Will','With','The','Www','Your','Is','Am','Some','You','Your','Was'}\n",
        "    rmdStopWordsLn = ' '.join(w for w in lineIn.split() if w.lower() not in stopWords)\n",
        "    return rmdStopWordsLn\n",
        "\n",
        "#preprocessing the text and remove special characters\n",
        "def preprocessText(lineIn):\n",
        "    lineInLower=lineIn.lower()\n",
        "    lineInRmdSplChars=lineInLower.replace('.',' ').replace(';',' ').replace(',',' ').replace('?',' ').replace('!',' ').replace(':',' ')\n",
        "    return lineInRmdSplChars\n",
        "\n",
        "#Divide the given text into lines\n",
        "def getAllLines(lineIn):\n",
        "    lineInReplcByPeriod=lineIn.replace('.','.§').replace(';',';§').replace(',',',§').replace('?','?§').replace('!','!§').replace('\\n','§')\n",
        "    linesOriginal=lineInReplcByPeriod.split('§')\n",
        "    linesOriginal2=[item for item in linesOriginal if len(item)>0 ]\n",
        "    return linesOriginal2\n",
        "\n",
        "#Identify the noun position\n",
        "def getNounPositions(type,tagged):\n",
        "    nounPosi={}\n",
        "    for item in tagged:\n",
        "        if item[1]==type:\n",
        "            nounPosi[item[0]]=-1\n",
        "    \n",
        "    for key in nounPosi.keys():\n",
        "        regExpression=r'\\b'+key.lower()+r'\\b'\n",
        "        nounsi=[m.start() for m in re.finditer(regExpression, lineIn.lower())]\n",
        "#        print(key,nounsi)\n",
        "        nounPosi[key]=nounsi\n",
        "    return nounPosi\n",
        "#Identify the pronoun position\n",
        "def getProNounPositions(tagged):\n",
        "    proNounPosi={}\n",
        "    for item in tagged:\n",
        "        if item[1]=='PRP': #or item[1]=='PRP$':\n",
        "            proNounPosi[item[0].lower()]=-1\n",
        "    \n",
        "    for key in proNounPosi.keys():\n",
        "        regExpression=r'\\b'+key.lower()+r'\\b'\n",
        "        pronounsi=[m.start() for m in re.finditer(regExpression, lineIn.lower())]\n",
        "#        print(key,pronounsi)\n",
        "        proNounPosi[key]=pronounsi\n",
        "    return proNounPosi\n",
        "\n",
        "#Obtain nearest previous noun\n",
        "def getNearestPreviousNoun(NNP,posiOfPronoun):\n",
        "#    print('\\t',NNP)    \n",
        "    minimumDiff=len(lineIn)\n",
        "    nearKey=''\n",
        "    for keyNNP in NNP.keys():\n",
        "        for posNoun in NNP[keyNNP]:\n",
        "            if(posiOfPronoun>posNoun):\n",
        "#                print('\\t',posiOfPronoun-posNoun)\n",
        "                if(minimumDiff>(posiOfPronoun-posNoun)):\n",
        "                    minimumDiff=posiOfPronoun-posNoun\n",
        "                    nearKey=keyNNP\n",
        "#    print('\\t near key=',nearKey)\n",
        "    return nearKey\n",
        "\n",
        "#Replace pronoun by noun\n",
        "def pronounReplaceWithNearNoun(lineIn,PRP,NNP):\n",
        "    replacePRP=[]       \n",
        "    for key in PRP.keys():            \n",
        "        for pos in PRP[key]:\n",
        "            print('---------',key,'------',pos ,'-----')\n",
        "            nearNoun=getNearestPreviousNoun(NNP,pos)\n",
        "            replacePRP.append((key,pos,nearNoun))  \n",
        "#    print(PRP)\n",
        "#    print(replacePRP)\n",
        "    \n",
        "    replacePRP=sorted(replacePRP,key=lambda x:(-x[1],x[0],x[2]))\n",
        "    lineInReplacePronn=lineIn\n",
        "    for prpRep in replacePRP:\n",
        "        lineInReplacePronn=lineInReplacePronn[:prpRep[1]]+prpRep[2]+lineInReplacePronn[prpRep[1]+len(prpRep[0]):]\n",
        "    return lineInReplacePronn\n",
        "\n",
        "#Based on weightage obtain the priority of lines\n",
        "def obtainPriorotyOfALine(wtForLine):\n",
        "    orderdLinesByWt=np.argsort(wtForLine)\n",
        "    orderdLinesByWt=orderdLinesByWt[::-1]\n",
        "    priority=[0]*len(wtForLine)\n",
        "    \n",
        "    for i in range(len(wtForLine)):\n",
        "#        print(i,wtForLine[i],orderdLinesByWt[i])\n",
        "        priority[orderdLinesByWt[i]]=i\n",
        "    \n",
        "    sentWtAndPriority=[]\n",
        "    \n",
        "    for i in range(len(wtForLine)):\n",
        "        sentWtAndPriority.append((wtForLine[i],priority[i]))\n",
        "    \n",
        "    return sentWtAndPriority\n",
        "#Construct summary by extraction method\n",
        "def obtainSummary(lineForCalc,lineForExtract,percentageOfSummary):\n",
        "    wtForLine=[0]*len(lineForCalc)\n",
        "    print('Calcualting wt for lines......')\n",
        "    for li in range(len(lineForCalc)):\n",
        "    #    print('\\t'+linesOriginal2[li])\n",
        "        wtForLn=0.0\n",
        "        preproccdLn2=preprocessText(lineForCalc[li])\n",
        "        wInL=preproccdLn2.split()\n",
        "        for w in wInL:\n",
        "            w=preprocessText(w)\n",
        "            if w in list(freqOfWords.keys()):\n",
        "    #            print('\\t\\t'+w+' '+str(freqOfWords[w]))\n",
        "                wtForLn=wtForLn+freqOfWords[w]\n",
        "        wtForLine[li]=(wtForLn/len(wInL))\n",
        "    #        print('\\t\\t'+lineForCalc[li]+' $'+str(wtForLine[li]))\n",
        "    \n",
        "    sentWtAndPriority=obtainPriorotyOfALine(wtForLine)\n",
        "    print(sentWtAndPriority)\n",
        "    numOfLinesInSummary=int((percentageOfSummary*len(lineForCalc))/100)\n",
        "    reducedSummary=[]\n",
        "    for li in range(len(lineForExtract)):\n",
        "        if(sentWtAndPriority[li][1]<numOfLinesInSummary):\n",
        "    #            print(li,sentWtAndPriority[li])\n",
        "            reducedSummary.append(lineForExtract[li])\n",
        "    return reducedSummary\n",
        "\n",
        "#remove week nm and month name\n",
        "def removeWeekNmMonthNm(NNP):\n",
        "    entries = ('january','february','march','april','may','june','july','august','september','october','november','december','monday','tuesday','wednesday','thursday','friday','saturday','sunday')\n",
        "    delKeys=[]\n",
        "    for key in NNP.keys():\n",
        "        if key.lower() in entries:\n",
        "            print(key)\n",
        "            delKeys.append(key)\n",
        "    \n",
        "    for key in delKeys:\n",
        "        del NNP[key]\n",
        "    return NNP\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uXMWneDdMPWI",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "nltk.download('punkt')\n",
        "nltk.download('averaged_perceptron_tagger')\n",
        "from gensim.summarization.summarizer import summarize"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Lyq5KPt7QL8h",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "pip install gensim"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ebsVe-5kQRot",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "lineIn = \"My name is Sourav and I am from India. My age is 24. I am currently stying in Passau. I am studing Mobile and embedded system. My father name is Uttam Kumar gupta. He is a business man. He own a shop. His work is of Photography. \"\n",
        "print( gensim.summarization.summarizer.summarize(lineIn, ratio=0.5, word_count=None, split=False))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VUch04lCL3kf",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "\n",
        "lineIn = \"My age is 24. My father name is Uttam Kumar gupta. My name is Sourav and I am from India.\"\n",
        "print( gensim.summarization.summarizer.summarize(lineIn,ratio=0.5, word_count=None, split=False))\n",
        "nt=len(lineIn.split())\n",
        "freqOfWords = Counter(re.split(r'\\s+',re.sub(r'[.,;\\-!?]','',lineIn)))\n",
        "for word, freq in freqOfWords.items():   \n",
        "  freqOfWords[word]=freqOfWords[word]/nt\n",
        "\n",
        "tagged=getTagsForWords(lineIn)\n",
        "NNP=getNounPositions('NNP',tagged)\n",
        "NNP=removeWeekNmMonthNm(NNP)\n",
        "#NN=getNounPositions('NN',tagged)\n",
        "#NNS=getNounPositions('NNS',tagged)\n",
        "PRP=getProNounPositions(tagged)\n",
        "\n",
        "linesOriginal2=getAllLines(lineIn)\n",
        "lineInReplacePronn=pronounReplaceWithNearNoun(lineIn,PRP,NNP)\n",
        "linesReplacedPronn2=getAllLines(lineInReplacePronn)\n",
        "print(linesReplacedPronn2)\n",
        "print(percentageOfSummary)\n",
        "\n",
        "#perform the text summarization without pronoun replacement\n",
        "reducedSummaryWithoutReplc=obtainSummary(linesOriginal2,linesOriginal2,percentageOfSummary)\n",
        "#perform the text summarization with pronoun replacement\n",
        "reducedSummaryWithReplc=obtainSummary(linesReplacedPronn2,linesOriginal2,percentageOfSummary)\n",
        "\n",
        "\n",
        "#text summeriazation.\n",
        "\n",
        "print(reducedSummaryWithReplc)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "Az8vX-psrEKR",
        "colab": {}
      },
      "source": [
        "doc = nlp(lineIn)\n",
        "\n",
        "from prettytable import PrettyTable\n",
        "t = PrettyTable(['text', 'token.head.text','token.head.tag_','child','pos_','tag_','dep_','shape_',])\n",
        " \n",
        "for token in doc:\n",
        "  child =  ([child for child in token.children])\n",
        "  t.add_row([token.text,token.head.text, spacy.explain(token.head.tag_), str(child), spacy.explain(token.pos_),spacy.explain(token.tag_) ,spacy.explain(token.dep_) ,\n",
        "          spacy.explain(token.shape_)])\n",
        "    \n",
        "print(t)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "xpfWKgnkqaXN",
        "colab": {}
      },
      "source": [
        "for ent in doc.ents:\n",
        "    print(ent.text, ent.start_char, ent.end_char, ent.label_)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bcJY3ybppfC2",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "root = [token for token in doc if token.head == token][0]\n",
        "subject = list(root.lefts)[0]\n",
        "for descendant in subject.subtree:\n",
        "    assert subject is descendant or subject.is_ancestor(descendant)\n",
        "    print(descendant.text, descendant.dep_, descendant.n_lefts,\n",
        "            descendant.n_rights,\n",
        "            [ancestor.text for ancestor in descendant.ancestors])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cdK63eRMww9D",
        "colab_type": "text"
      },
      "source": [
        "### **Sentiment**\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "otUO23Nmw0zP",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from nltk.classify import NaiveBayesClassifier\n",
        "from nltk.corpus import subjectivity\n",
        "from nltk.sentiment import SentimentAnalyzer\n",
        "from nltk.sentiment.util import *\n",
        "from nltk.sentiment.vader import SentimentIntensityAnalyzer"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "slsETCzyxoLk",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "nltk.download('vader_lexicon')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0E5xNmrmSZif",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wt4TRaO2w871",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        " tricky_sentences = [\n",
        "    \"i really liked the movie and had fun. Sentiment\",\n",
        "    \"VADER sentiment analysis is the shit.\",\n",
        "    \"Sentiment analysis has never been good.\",\n",
        "    \"Sentiment analysis with VADER has never been this good.\",\n",
        "    \"Warren Beatty has never been so entertaining.\",\n",
        "    \"I won't say that the movie is astounding and I wouldn't claim that \\\n",
        "    the movie is too banal either.\",\n",
        "    \"I like to hate Michael Bay films, but I couldn't fault this one\",\n",
        "    \"It's one thing to watch an Uwe Boll film, but another thing entirely \\\n",
        "    to pay for it\",\n",
        "    \"The movie was too good\",\n",
        "    \"This movie was actually neither that funny, nor super witty.\",\n",
        "    \"This movie doesn't care about cleverness, wit or any other kind of \\\n",
        "    intelligent humor.\",\n",
        "    \"Those who find ugly meanings in beautiful things are corrupt without \\\n",
        "    being charming.\",\n",
        "    \"There are slow and repetitive parts, BUT it has just enough spice to \\\n",
        "    keep it interesting.\",\n",
        "    \"The script is not fantastic, but the acting is decent and the cinematography \\\n",
        "    is EXCELLENT!\",\n",
        "    \"Roger Dodger is one of the most compelling variations on this theme.\",\n",
        "    \"Roger Dodger is one of the least compelling variations on this theme.\",\n",
        "    \"Roger Dodger is at least compelling as a variation on the theme.\",\n",
        "    \"they fall in love with the product\",\n",
        "    \"but then it breaks\",\n",
        "    \"usually around the time the 90 day warranty expires\",\n",
        "    \"this is  excellent\",\n",
        "    \"However, Mr. Carter solemnly argues, his client carried out the kidnapping \\\n",
        "    under orders and in the ''least offensive way possible.''\"\n",
        " ]\n",
        " sentences.extend(tricky_sentences)\n",
        " sid = SentimentIntensityAnalyzer()\n",
        " for sentence in tricky_sentences :\n",
        "     print(sentence)\n",
        "     ss = sid.polarity_scores(sentence)\n",
        "     for k in sorted(ss):\n",
        "         print('{0}: {1}, '.format(k, ss[k]), end='')\n",
        "     print()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "V8x_2kFgxZjl",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pRLzfgCuM0Ts",
        "colab_type": "text"
      },
      "source": [
        "### **Keras**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "q4akxOA7M9jj",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from numpy import array\n",
        "from keras.preprocessing.text import one_hot\n",
        "docs = ['Gut gemacht',\n",
        "        'Gute arbeit',\n",
        "        'Super idee',\n",
        "        'Perfekt erledigt',\n",
        "        'exzellent',\n",
        "        'naja',\n",
        "        'Schwache arbeit.',\n",
        "        'Nicht gut',\n",
        "        'Miese arbeit.',\n",
        "        'Hätte es besser machen können.']\n",
        "# integer encode the documents\n",
        "vocab_size = 50\n",
        "encoded_docs = [one_hot(d, vocab_size) for d in docs]\n",
        "print(encoded_docs)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9AZ-rzYwNAMf",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}