{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Learn_Embeddings_from_scratch.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "mount_file_id": "1hoHs2TyG1MaTyvq4FV5UgEg_0tl_WgIB",
      "authorship_tag": "ABX9TyM6A4PWWoIAmLuvXcLRQEI1",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/sourav044/Python/blob/master/Learn_Embeddings_from_scratch.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QwYfyU0FhxbP",
        "outputId": "2c8481cb-fecb-4ce0-fe6a-6d90e81e2f63"
      },
      "source": [
        "import re\n",
        "import string\n",
        "import numpy as np # linear algebra\n",
        "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
        "import nltk\n",
        "from tensorflow.python.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.python.keras.preprocessing.sequence import pad_sequences\n",
        "from IPython.display import display, HTML\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.manifold import TSNE\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "nltk.download('stopwords')\n",
        "nltk.download('punkt')\n",
        "nltk.download('wordnet')\n",
        "stop_words = set(stopwords.words('english'))\n",
        "lemmatizer = WordNetLemmatizer()"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rTw4Df5QiY1k",
        "outputId": "b60c560b-6d2f-48d9-efec-64255bc1b69f"
      },
      "source": [
        "# Download dataset \n",
        "# !wget https://filebin.net/fby5uc7dccvazp6y/wiki.zip\n",
        "# !unzip wiki.zip"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "--2021-06-19 17:09:57--  https://filebin.net/fby5uc7dccvazp6y/wiki.zip\n",
            "Resolving filebin.net (filebin.net)... 185.47.40.36, 2a02:c0:2f0:700:f816:3eff:fe73:c194\n",
            "Connecting to filebin.net (filebin.net)|185.47.40.36|:443... connected.\n",
            "HTTP request sent, awaiting response... 303 See Other\n",
            "Location: https://situla.bitbit.net/filebin/ebe325cb0188d2c9e1eb5eda63b4596f0090f8e36b697b7f72e7c26ac7043645/1ef12115ca77ce683936767ed9676d1820cde8eef7c2b092229e15d0b4eee1d5?X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=HZXB1J7T0UN34UN512IW%2F20210619%2Fus-east-1%2Fs3%2Faws4_request&X-Amz-Date=20210619T170957Z&X-Amz-Expires=60&X-Amz-SignedHeaders=host&response-cache-control=max-age%3D60%2C%20must-revalidate&response-content-disposition=filename%3D%22wiki.zip%22&response-content-type=application%2Fzip&X-Amz-Signature=33d187dfc3e85cc5c13d3e502eaae6bc36dc296d3f0b59cd3ea20dc28dc953eb [following]\n",
            "--2021-06-19 17:09:57--  https://situla.bitbit.net/filebin/ebe325cb0188d2c9e1eb5eda63b4596f0090f8e36b697b7f72e7c26ac7043645/1ef12115ca77ce683936767ed9676d1820cde8eef7c2b092229e15d0b4eee1d5?X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=HZXB1J7T0UN34UN512IW%2F20210619%2Fus-east-1%2Fs3%2Faws4_request&X-Amz-Date=20210619T170957Z&X-Amz-Expires=60&X-Amz-SignedHeaders=host&response-cache-control=max-age%3D60%2C%20must-revalidate&response-content-disposition=filename%3D%22wiki.zip%22&response-content-type=application%2Fzip&X-Amz-Signature=33d187dfc3e85cc5c13d3e502eaae6bc36dc296d3f0b59cd3ea20dc28dc953eb\n",
            "Resolving situla.bitbit.net (situla.bitbit.net)... 87.238.33.7, 87.238.33.8, 2a02:c0::8, ...\n",
            "Connecting to situla.bitbit.net (situla.bitbit.net)|87.238.33.7|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 5196996 (5.0M) [application/zip]\n",
            "Saving to: ‘wiki.zip.1’\n",
            "\n",
            "wiki.zip.1          100%[===================>]   4.96M  16.1MB/s    in 0.3s    \n",
            "\n",
            "2021-06-19 17:09:58 (16.1 MB/s) - ‘wiki.zip.1’ saved [5196996/5196996]\n",
            "\n",
            "Archive:  wiki.zip\n",
            "replace cleaned_hm.csv? [y]es, [n]o, [A]ll, [N]one, [r]ename: y\n",
            "  inflating: cleaned_hm.csv          \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RD5n4EWRojRy",
        "outputId": "d011ff93-cf61-439d-9c37-aa188d0c97fc"
      },
      "source": [
        "# Download dataset \n",
        "import tarfile\n",
        "!wget clone https://www.cs.upc.edu/~nlp/wikicorpus/raw.en.tgz\n",
        "tar = tarfile.open('raw.en.tgz', \"r:gz\")\n",
        "tar.extractall()\n",
        "tar.close()"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "--2021-06-19 17:12:15--  http://clone/\n",
            "Resolving clone (clone)... failed: Name or service not known.\n",
            "wget: unable to resolve host address ‘clone’\n",
            "--2021-06-19 17:12:15--  https://www.cs.upc.edu/~nlp/wikicorpus/raw.en.tgz\n",
            "Resolving www.cs.upc.edu (www.cs.upc.edu)... 147.83.20.36\n",
            "Connecting to www.cs.upc.edu (www.cs.upc.edu)|147.83.20.36|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 1346378932 (1.3G) [application/x-gzip]\n",
            "Saving to: ‘raw.en.tgz.1’\n",
            "\n",
            "raw.en.tgz.1        100%[===================>]   1.25G  7.72MB/s    in 2m 54s  \n",
            "\n",
            "2021-06-19 17:15:10 (7.37 MB/s) - ‘raw.en.tgz.1’ saved [1346378932/1346378932]\n",
            "\n",
            "FINISHED --2021-06-19 17:15:10--\n",
            "Total wall clock time: 2m 55s\n",
            "Downloaded: 1 files, 1.3G in 2m 54s (7.37 MB/s)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qlNAsbC3baun",
        "outputId": "dd26927e-0cfc-44dc-d289-6a4cbc0bde8e"
      },
      "source": [
        "import glob, os\n",
        "file_data = ''\n",
        "for file, file_count in zip(glob.glob(\"englishText*\"), range(1)):\n",
        "  print(file_count, file)\n",
        "  with open(file, 'r', encoding='utf-8', errors='ignore') as file:\n",
        "    contents =   file.read()\n",
        "    search_word =  ['king', 'queen', 'men', 'women']\n",
        "    if any(word in contents for word in search_word):\n",
        "      file_data += contents"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0 englishText_1110000_1120000\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dAmyehuZhzl6"
      },
      "source": [
        "### Pre-processing of the text\n",
        "1. Split text into sentences\n",
        "2. Convert sentence into lowercase \n",
        "3. Remove special char \n",
        "4. Remove puntuations\n",
        "4. Remove Stop Words\n",
        "5. Tokenize Words  (to be in future use: 'PTBTokenizer')"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yYAKayM9h3-V"
      },
      "source": [
        "# Read the wiki data-set \n",
        "# file_data = file.read()\n",
        "\n",
        "# split file data i.e text into sentences \n",
        "sentence_collection =list()\n",
        "\n",
        "sentences = nltk.sent_tokenize(file_data)\n",
        "\n",
        "for sentence in sentences:\n",
        "    sentence = sentence.lower()\n",
        "\n",
        "    pattern = re.compile('http[s]?://(?:[a-zA-Z]|[0-9]|[$-_@.&+]|[!*\\(\\),]|(?:%[0-9a-fA-F][0-9a-fA-F]))+')\n",
        "    sentence = pattern.sub('', sentence)\n",
        "    \n",
        "    sentence = re.sub(r\"i'm\", \"i am\", sentence)\n",
        "    sentence = re.sub(r\"he's\", \"he is\", sentence)\n",
        "    sentence = re.sub(r\"she's\", \"she is\", sentence)\n",
        "    sentence = re.sub(r\"that's\", \"that is\", sentence)        \n",
        "    sentence = re.sub(r\"what's\", \"what is\", sentence)\n",
        "    sentence = re.sub(r\"where's\", \"where is\", sentence) \n",
        "    sentence = re.sub(r\"\\'ll\", \" will\", sentence)  \n",
        "    sentence = re.sub(r\"\\'ve\", \" have\", sentence)  \n",
        "    sentence = re.sub(r\"\\'re\", \" are\", sentence)\n",
        "    sentence = re.sub(r\"\\'d\", \" would\", sentence)\n",
        "    sentence = re.sub(r\"\\'ve\", \" have\", sentence)\n",
        "    sentence = re.sub(r\"won't\", \"will not\", sentence)\n",
        "    sentence = re.sub(r\"don't\", \"do not\", sentence)\n",
        "    sentence = re.sub(r\"did't\", \"did not\", sentence)\n",
        "    sentence = re.sub(r\"can't\", \"can not\", sentence)\n",
        "    sentence = re.sub(r\"it's\", \"it is\", sentence)\n",
        "    sentence = re.sub(r\"couldn't\", \"could not\", sentence)\n",
        "    sentence = re.sub(r\"have't\", \"have not\", sentence)\n",
        "\n",
        "    sentence = re.sub(r\"[,.\\\"!@#$%^&*(){}?/;`~:<>+=-]\", \"\", sentence)    \n",
        "    words_tokens = word_tokenize(sentence)\n",
        "\n",
        "    st_punct = str.maketrans('', '', string.punctuation)\n",
        "    words_tokens = [word.translate(st_punct) for word in words_tokens]\n",
        "    words_tokens = [word for word in words_tokens if word.isalpha()]\n",
        "    \n",
        "    sentence = [lemmatizer.lemmatize(word) for word in words_tokens if not word in stop_words]\n",
        "\n",
        "    sentence_collection.append(sentence)\n",
        "    \n",
        "file_data = None # Variable dispose \n",
        "sentences = None\n",
        "del file_data, sentences"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Fzg5KwTniAiO"
      },
      "source": [
        "wordtoken_obj = Tokenizer()\n",
        "# fit_on_texts Updates internal vocabulary based on a list of texts. \n",
        "# This method creates the vocabulary index based on word frequency. \n",
        "# So if you give it something like,\n",
        "# \"The cat sat on the mat.\" It will create a dictionary s.t. word_index[\"the\"] = 1; word_index[\"cat\"] = 2 it is word -> index dictionary so every word gets a unique integer value. \n",
        "# 0 is reserved for padding. So lower integer means more frequent word (often the first few are stop words because they appear a lot).\n",
        "wordtoken_obj.fit_on_texts(sentence_collection)\n",
        "# texts_to_sequences Transforms each text in texts to a sequence of integers.\n",
        "# So it basically takes each word in the text and replaces it with its corresponding integer value from the word_index dictionary. \n",
        " \n",
        "word_sequences_int = wordtoken_obj.texts_to_sequences(sentence_collection)"
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0QMDCZMbvjXK"
      },
      "source": [
        "### Data-set values :"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "l2lPg9QgvaZf",
        "outputId": "af15e7f1-b5ce-425f-cb5f-f278a67e0d89"
      },
      "source": [
        "word_index = wordtoken_obj.word_index\n",
        "print(\"unique tokens - \"+str(len(word_index)))\n",
        "vocab_size = len(wordtoken_obj.word_index) + 1\n",
        "print('vocab_size - '+str(vocab_size))\n",
        "print(list(word_index.items())[:5])\n",
        "print(\"corpus\",list(word_index)[:5])"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "unique tokens - 160329\n",
            "vocab_size - 160330\n",
            "[('doc', 1), ('also', 2), ('endofarticle', 3), ('one', 4), ('first', 5)]\n",
            "corpus ['doc', 'also', 'endofarticle', 'one', 'first']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "S2XStZQN09FK"
      },
      "source": [
        "\n",
        "### Vector Encoding\n",
        "\n",
        "One hot encoding & Neighbour One hot ncoding\n",
        "\n",
        "---\n",
        "\n",
        "Current word = paris    <- Target word  \n",
        "Neighbour words = (way,to)  <- Context word \n",
        "\n",
        " \n",
        "<center>\n",
        "current_word_one_hot_vector = [0, 0, 0, 0, 0, 1, 0, 0, 0]  <br/>\n",
        "neighbour_word_one_hot_vector = [0, 0, 0, 1, 0, 1, 0, 0, 0]\n",
        "</center>\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4g2uqoxEwt74"
      },
      "source": [
        "def get_one_hot_vectors(target_word,context_words,vocab_size,word_to_index):\n",
        "    \n",
        "    #Create an array of size = vocab_size filled with zeros\n",
        "    trgt_word_vector = np.zeros(vocab_size)\n",
        "    \n",
        "    #Get the index of the target_word according to the dictionary word_to_index. \n",
        "    #If target_word = best, the index according to the dictionary word_to_index is 0. \n",
        "    #So the one hot vector will be [1, 0, 0, 0, 0, 0, 0, 0, 0]\n",
        "    index_of_word_dictionary = word_to_index.get(target_word) \n",
        "    \n",
        "    #Set the index to 1\n",
        "    trgt_word_vector[index_of_word_dictionary] = 1\n",
        "    \n",
        "    #Repeat same steps for context_words but in a loop\n",
        "    ctxt_word_vector = np.zeros(vocab_size)\n",
        "    \n",
        "    \n",
        "    for word in context_words:\n",
        "        index_of_word_dictionary = word_to_index.get(word) \n",
        "        ctxt_word_vector[index_of_word_dictionary] = 1\n",
        "        \n",
        "    return trgt_word_vector,ctxt_word_vector"
      ],
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "m1VDtlhS22ET"
      },
      "source": [
        " def generate_training_data(corpus,window_size,vocab_size,word_to_index,length_of_corpus,sample=None):\n",
        "\n",
        "    training_data =  []\n",
        "    training_sample_words =  []\n",
        "    for i,word in enumerate(corpus):\n",
        "\n",
        "        index_target_word = i\n",
        "        target_word = word\n",
        "        context_words = []\n",
        "\n",
        "        #when target word is the first word\n",
        "        if i == 0:  \n",
        "\n",
        "            # trgt_word_index:(0), ctxt_word_index:(1,2)\n",
        "            context_words = [corpus[x] for x in range(i + 1 , window_size + 1)] \n",
        "\n",
        "\n",
        "        #when target word is the last word\n",
        "        elif i == len(corpus)-1:\n",
        "\n",
        "            # trgt_word_index:(9), ctxt_word_index:(8,7), length_of_corpus = 10\n",
        "            context_words = [corpus[x] for x in range(length_of_corpus - 2 ,length_of_corpus -2 - window_size  , -1 )]\n",
        "\n",
        "        #When target word is the middle word\n",
        "        else:\n",
        "\n",
        "            #Before the middle target word\n",
        "            before_target_word_index = index_target_word - 1\n",
        "            for x in range(before_target_word_index, before_target_word_index - window_size , -1):\n",
        "                if x >=0:\n",
        "                    context_words.extend([corpus[x]])\n",
        "\n",
        "            #After the middle target word\n",
        "            after_target_word_index = index_target_word + 1\n",
        "            for x in range(after_target_word_index, after_target_word_index + window_size):\n",
        "                if x < len(corpus):\n",
        "                    context_words.extend([corpus[x]])\n",
        "\n",
        "\n",
        "        trgt_word_vector,ctxt_word_vector = get_one_hot_vectors(target_word,context_words,vocab_size,word_to_index)\n",
        "        training_data.append([trgt_word_vector,ctxt_word_vector])   \n",
        "        \n",
        "        if sample is not None:\n",
        "            training_sample_words.append([target_word,context_words])   \n",
        "        \n",
        "    return training_data,training_sample_words"
      ],
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mT_3MoWFFgqf"
      },
      "source": [
        "window_size = 2\n",
        "corpus = list(word_index)\n",
        "training_data,training_sample_words = generate_training_data(corpus,window_size,vocab_size,word_index,vocab_size+1,'yes')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JLI5a5HGiGl_"
      },
      "source": [
        "## Gensim"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tAsLkVaiiHW7"
      },
      "source": [
        "import gensim\n",
        "model = gensim.models.Word2Vec(sentences=sentence_collection,  window=5, workers=4, min_count=1, sg=0) #sg= 1:skip-gram 0:cbow\n",
        "# model.train(sentence_collection,total_examples=len(sentence_collection),epochs=50)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hMtbAKrmiMiO"
      },
      "source": [
        "model.wv.most_similar (positive='woman')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-fWxj9BSnPvk"
      },
      "source": [
        "model.wv.most_similar(positive=['woman', 'king'], negative=['man'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Dz_mHU2dDu2n"
      },
      "source": [
        "# Glove"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nrWS-MtVr5g9"
      },
      "source": [
        "!pip install glove-python-binary"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BUsx54Gy5WOT"
      },
      "source": [
        "#importing the glove library\n",
        "from glove import Corpus, Glove\n",
        "# creating a corpus object\n",
        "corpus = Corpus() \n",
        "#training the corpus to generate the co occurence matrix which is used in GloVe\n",
        "corpus.fit(sentence_collection, window=10)\n",
        "#creating a Glove object which will use the matrix created in the above lines to create embeddings\n",
        "#We can set the learning rate as it uses Gradient Descent and number of components\n",
        "glove = Glove(no_components=5, learning_rate=0.05)\n",
        " \n",
        "glove.fit(corpus.matrix, epochs=50, no_threads=4, verbose=True)\n",
        "glove.add_dictionary(corpus.dictionary)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sbpAgGHQ2mQY"
      },
      "source": [
        "glove.most_similar('king')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jb4Iic-Py5nT"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}