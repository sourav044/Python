{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Gensim.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/sourav044/Python/blob/master/NLP_GENSIM.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jhQuxZjTzrPz",
        "colab_type": "code",
        "outputId": "1fb22232-0ad6-4c5a-b127-8051cab2766e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 332
        }
      },
      "source": [
        "!pip install gensim"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: gensim in /usr/local/lib/python3.6/dist-packages (3.6.0)\n",
            "Requirement already satisfied: six>=1.5.0 in /usr/local/lib/python3.6/dist-packages (from gensim) (1.12.0)\n",
            "Requirement already satisfied: scipy>=0.18.1 in /usr/local/lib/python3.6/dist-packages (from gensim) (1.3.1)\n",
            "Requirement already satisfied: numpy>=1.11.3 in /usr/local/lib/python3.6/dist-packages (from gensim) (1.16.5)\n",
            "Requirement already satisfied: smart-open>=1.2.1 in /usr/local/lib/python3.6/dist-packages (from gensim) (1.8.4)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.6/dist-packages (from smart-open>=1.2.1->gensim) (2.21.0)\n",
            "Requirement already satisfied: boto>=2.32 in /usr/local/lib/python3.6/dist-packages (from smart-open>=1.2.1->gensim) (2.49.0)\n",
            "Requirement already satisfied: boto3 in /usr/local/lib/python3.6/dist-packages (from smart-open>=1.2.1->gensim) (1.9.250)\n",
            "Requirement already satisfied: chardet<3.1.0,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests->smart-open>=1.2.1->gensim) (3.0.4)\n",
            "Requirement already satisfied: idna<2.9,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests->smart-open>=1.2.1->gensim) (2.8)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests->smart-open>=1.2.1->gensim) (2019.9.11)\n",
            "Requirement already satisfied: urllib3<1.25,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests->smart-open>=1.2.1->gensim) (1.24.3)\n",
            "Requirement already satisfied: botocore<1.13.0,>=1.12.250 in /usr/local/lib/python3.6/dist-packages (from boto3->smart-open>=1.2.1->gensim) (1.12.250)\n",
            "Requirement already satisfied: s3transfer<0.3.0,>=0.2.0 in /usr/local/lib/python3.6/dist-packages (from boto3->smart-open>=1.2.1->gensim) (0.2.1)\n",
            "Requirement already satisfied: jmespath<1.0.0,>=0.7.1 in /usr/local/lib/python3.6/dist-packages (from boto3->smart-open>=1.2.1->gensim) (0.9.4)\n",
            "Requirement already satisfied: python-dateutil<3.0.0,>=2.1; python_version >= \"2.7\" in /usr/local/lib/python3.6/dist-packages (from botocore<1.13.0,>=1.12.250->boto3->smart-open>=1.2.1->gensim) (2.5.3)\n",
            "Requirement already satisfied: docutils<0.16,>=0.10 in /usr/local/lib/python3.6/dist-packages (from botocore<1.13.0,>=1.12.250->boto3->smart-open>=1.2.1->gensim) (0.15.2)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wUTaLTnazoAp",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from gensim.test.utils import common_texts\n",
        "from gensim.models import Word2Vec\n",
        "from gensim.similarities import WmdSimilarity\n",
        "text = ['human', 'interface', 'computer']\n",
        "model = Word2Vec(common_texts, size=20, min_count=1)  # train word-vectors\n",
        "\n",
        "index = WmdSimilarity(text, model)\n",
        "# Make query.\n",
        "query = ['trees']\n",
        "sims = index[query]\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Z2-cxm0e1nlr",
        "colab_type": "code",
        "outputId": "cb8cea1a-59ad-40b0-b236-93bda709bdcc",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 89
        }
      },
      "source": [
        "print(common_texts)\n",
        "print(model)\n",
        "print(sims)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[['human', 'interface', 'computer'], ['survey', 'user', 'computer', 'system', 'response', 'time'], ['eps', 'user', 'interface', 'system'], ['system', 'human', 'system', 'eps'], ['user', 'response', 'time'], ['trees'], ['graph', 'trees'], ['graph', 'minors', 'trees'], ['graph', 'minors', 'survey']]\n",
            "Word2Vec(vocab=12, size=20, alpha=0.025)\n",
            "[0. 0. 0.]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oXNlVMY431pE",
        "colab_type": "code",
        "outputId": "0ef118ec-03b8-4f02-ecc4-45b19330ce04",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 72
        }
      },
      "source": [
        "from gensim.test.utils import common_texts\n",
        "from gensim.models import Word2Vec\n",
        "from gensim.similarities import WmdSimilarity\n",
        "\n",
        "model = Word2Vec(common_texts, size=20, min_count=1)  # train word-vectors\n",
        "index = WmdSimilarity(common_texts, model)\n",
        " # Make query.\n",
        "query = ['plant']\n",
        "sims = index[query]\n",
        "print(common_texts)\n",
        "print(sims)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[['human', 'interface', 'computer'], ['survey', 'user', 'computer', 'system', 'response', 'time'], ['eps', 'user', 'interface', 'system'], ['system', 'human', 'system', 'eps'], ['user', 'response', 'time'], ['trees'], ['graph', 'trees'], ['graph', 'minors', 'trees'], ['graph', 'minors', 'survey']]\n",
            "[0. 0. 0. 0. 0. 0. 0. 0. 0.]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "k87koUW49mKc",
        "colab_type": "text"
      },
      "source": [
        "NOW starting the debug "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nIdwOjiv-FL6",
        "colab_type": "code",
        "outputId": "9518747f-c572-4a45-cb52-6960a1bafb32",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 211
        }
      },
      "source": [
        "pip install PyPDF2"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting PyPDF2\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/b4/01/68fcc0d43daf4c6bdbc6b33cc3f77bda531c86b174cac56ef0ffdb96faab/PyPDF2-1.26.0.tar.gz (77kB)\n",
            "\u001b[K     |████████████████████████████████| 81kB 3.8MB/s \n",
            "\u001b[?25hBuilding wheels for collected packages: PyPDF2\n",
            "  Building wheel for PyPDF2 (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for PyPDF2: filename=PyPDF2-1.26.0-cp36-none-any.whl size=61085 sha256=351d512febd9d08aa3ec1cf52f0adf906113dea0d69181f9fdb20b4b35c988fe\n",
            "  Stored in directory: /root/.cache/pip/wheels/53/84/19/35bc977c8bf5f0c23a8a011aa958acd4da4bbd7a229315c1b7\n",
            "Successfully built PyPDF2\n",
            "Installing collected packages: PyPDF2\n",
            "Successfully installed PyPDF2-1.26.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Vb41P3B0kPst",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import nltk\n",
        "from nltk.tokenize import sent_tokenize"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YH_ToGs4-ROx",
        "colab_type": "code",
        "outputId": "6ffa9984-a1a9-4e43-8da0-f1b0a4f4bd8b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 69
        }
      },
      "source": [
        "nltk.download('stopwords') "
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nYZOlK3U-32D",
        "colab_type": "code",
        "outputId": "dc8406da-737b-4c84-cb26-9f44f4fafad8",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        }
      },
      "source": [
        "import spacy.cli\n",
        "lang = spacy.cli.download(\"en_core_web_sm\")\n",
        "nlp = spacy.load(\"en_core_web_sm\")\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
            "You can now load the model via spacy.load('en_core_web_sm')\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TXLCNEZ49ygl",
        "colab_type": "code",
        "outputId": "e881c086-aef0-4e09-aa76-5bc244fff539",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "from collections import Counter\n",
        "import nltk\n",
        "from nltk.tokenize import sent_tokenize\n",
        "from keras.preprocessing.text import Tokenizer \n",
        "import spacy\n",
        "import PyPDF2\n",
        "import keras\n",
        "import tensorflow as tf\n",
        "import gensim\n",
        "from nltk.corpus import stopwords"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using TensorFlow backend.\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-FVkZLUL7xmz",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "def combine(sub_keys, keywords_splits, lb, mb, ub):\n",
        "    whitespace = ' '\n",
        "    while mb != ub:\n",
        "        keywords_splits.append(whitespace.join(sub_keys[lb: mb]))\n",
        "        keywords_splits.append(whitespace.join(sub_keys[mb: ub]))\n",
        "        mb += 1\n",
        "    del sub_keys[0]\n",
        "    if len(sub_keys) > 2:\n",
        "        combine(sub_keys, keywords_splits, 0, 1, len(sub_keys))\n",
        "\n",
        "\n",
        "def keywords_splitter(keywords, keywords_splits):\n",
        "\n",
        "    for key in keywords:\n",
        "        sub_keys = key.split()\n",
        "\n",
        "        if len(sub_keys) > 2:\n",
        "            combine(sub_keys, keywords_splits, 0, 1, len(sub_keys))\n",
        "\n",
        "\n",
        "def pre_query(question_query):\n",
        "\n",
        "    keywords = question_query.get_features()\n",
        "\n",
        "    keywords = [feat.lower() for feat in keywords]\n",
        "    whitespace = ' '\n",
        "    keywords_splits = whitespace.join(keywords).split()\n",
        "\n",
        "    keywords_splitter(keywords, keywords_splits)\n",
        "    keywords_splits = list(set(keywords_splits + keywords))\n",
        "\n",
        "    return keywords_splits\n",
        "\n",
        "\n",
        "def query2vec(query, dictionary):\n",
        "\n",
        "    print(\"Searching: {0}\".format(query))\n",
        "    corpus = dictionary.doc2bow(query)\n",
        "\n",
        "    return corpus\n",
        "\n",
        "\n",
        "def doc2vec(documents):\n",
        "         \n",
        "    texts = [[word for word in doc.lemma_.split() if word not in ['a','b']]for doc in documents]\n",
        "\n",
        "    frequency = Counter()\n",
        "    for sent in texts:\n",
        "        for token in sent:\n",
        "            frequency[token] += 1\n",
        "\n",
        "    \n",
        "    text_token = [[token for token in snipp]for snipp in texts]\n",
        "    \n",
        "   \n",
        "    texts = [[token for token in snipp if frequency[token] > -1]for snipp in texts]\n",
        "\n",
        "    dictionary = gensim.corpora.Dictionary(texts)\n",
        "    corpus = [dictionary.doc2bow(snipp) for snipp in texts]\n",
        "\n",
        "    return corpus, dictionary, texts, text_token\n",
        "\n",
        "\n",
        "def transform_vec(corpus, query_corpus):\n",
        "    lsidf = gensim.models.LsiModel(corpus)\n",
        "\n",
        "    corpus_lsidf = lsidf[corpus]\n",
        "    query_lsidf = lsidf[query_corpus]\n",
        "\n",
        "    return corpus_lsidf, query_lsidf\n",
        "\n",
        "\n",
        "def similarity(corpus_lsidf, query_lsidf):\n",
        "    index = gensim.similarities.SparseMatrixSimilarity(corpus_lsidf, num_features=200000)\n",
        "\n",
        "    simi = index[query_lsidf]\n",
        "\n",
        "    simi_sorted = sorted(enumerate(simi), key=lambda item: -item[1])\n",
        "    return simi_sorted\n",
        "\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Z7v077Qu7yZ_",
        "colab_type": "code",
        "outputId": "43a3b02e-7f07-467a-975a-6c922867340d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 55
        }
      },
      "source": [
        "text = \"My name is Sourav and I am from India. My age is 24. I am currently stying in Passau. I am studing Mobile and embedded system. My father name is Uttam Kumar gupta. He is a business man. He own a shop. His work is of Photography. \"\n",
        "en_doc = nlp(u'' + text)\n",
        "sentences = list(en_doc.sents)\n",
        "print(sentences)\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[My name is Sourav and I am from India., My age is 24., I am currently stying in Passau., I am studing Mobile and embedded system., My father name is Uttam Kumar gupta., He is a business man., He own a shop., His work is of Photography.]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZPGaNQtGj3pM",
        "colab_type": "text"
      },
      "source": [
        "### **1ST Remove Stop Word and Bag of word**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_bBCdfNF9q0b",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "corpus, dictionary, text, token = doc2vec(sentences)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hL0Cj4wSFss-",
        "colab_type": "code",
        "outputId": "11aa76eb-a501-4e48-b86b-4d6dee491643",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 89
        }
      },
      "source": [
        "print(text)\n",
        "print(token)\n",
        "print(dictionary)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[['-PRON-', 'name', 'be', 'Sourav', 'and', '-PRON-', 'be', 'from', 'India', '.'], ['-PRON-', 'age', 'be', '24', '.'], ['-PRON-', 'be', 'currently', 'stye', 'in', 'Passau', '.'], ['-PRON-', 'be', 'stud', 'Mobile', 'and', 'embedded', 'system', '.'], ['-PRON-', 'father', 'name', 'be', 'Uttam', 'Kumar', 'gupta', '.'], ['-PRON-', 'be', 'business', 'man', '.'], ['-PRON-', 'own', 'shop', '.'], ['-PRON-', 'work', 'be', 'of', 'Photography', '.']]\n",
            "[['-PRON-', 'name', 'be', 'Sourav', 'and', '-PRON-', 'be', 'from', 'India', '.'], ['-PRON-', 'age', 'be', '24', '.'], ['-PRON-', 'be', 'currently', 'stye', 'in', 'Passau', '.'], ['-PRON-', 'be', 'stud', 'Mobile', 'and', 'embedded', 'system', '.'], ['-PRON-', 'father', 'name', 'be', 'Uttam', 'Kumar', 'gupta', '.'], ['-PRON-', 'be', 'business', 'man', '.'], ['-PRON-', 'own', 'shop', '.'], ['-PRON-', 'work', 'be', 'of', 'Photography', '.']]\n",
            "Dictionary(29 unique tokens: ['-PRON-', '.', 'India', 'Sourav', 'and']...)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "a_S3D4YoGOH8",
        "colab_type": "code",
        "outputId": "fad98ca9-b2e4-46c9-9720-9e9afe8ab812",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 55
        }
      },
      "source": [
        "print(corpus)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[(0, 2), (1, 1), (2, 1), (3, 1), (4, 1), (5, 2), (6, 1), (7, 1)], [(0, 1), (1, 1), (5, 1), (8, 1), (9, 1)], [(0, 1), (1, 1), (5, 1), (10, 1), (11, 1), (12, 1), (13, 1)], [(0, 1), (1, 1), (4, 1), (5, 1), (14, 1), (15, 1), (16, 1), (17, 1)], [(0, 1), (1, 1), (5, 1), (7, 1), (18, 1), (19, 1), (20, 1), (21, 1)], [(0, 1), (1, 1), (5, 1), (22, 1), (23, 1)], [(0, 1), (1, 1), (24, 1), (25, 1)], [(0, 1), (1, 1), (5, 1), (26, 1), (27, 1), (28, 1)]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "x_SG8RK5ktdb",
        "colab_type": "text"
      },
      "source": [
        "### **transform_vec**\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3lPeUMdsnLzL",
        "colab_type": "code",
        "outputId": "bd705734-56c9-4222-ffd2-304962a28068",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "query_corpus = query2vec([\"What is my age.\",\"age\",\"24\",\"name\"], dictionary)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Searching: ['What is my age.', 'age', '24', 'name']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Wpa0RWK0p9zj",
        "colab_type": "code",
        "outputId": "f7aca446-8912-4847-ae20-141130340745",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 55
        }
      },
      "source": [
        "print(corpus)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[(0, 2), (1, 1), (2, 1), (3, 1), (4, 1), (5, 2), (6, 1), (7, 1)], [(0, 1), (1, 1), (5, 1), (8, 1), (9, 1)], [(0, 1), (1, 1), (5, 1), (10, 1), (11, 1), (12, 1), (13, 1)], [(0, 1), (1, 1), (4, 1), (5, 1), (14, 1), (15, 1), (16, 1), (17, 1)], [(0, 1), (1, 1), (5, 1), (7, 1), (18, 1), (19, 1), (20, 1), (21, 1)], [(0, 1), (1, 1), (5, 1), (22, 1), (23, 1)], [(0, 1), (1, 1), (24, 1), (25, 1)], [(0, 1), (1, 1), (5, 1), (26, 1), (27, 1), (28, 1)]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aheSOofDksr-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "corpus_lsidf, query_lsidf = transform_vec(corpus, query_corpus)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jGCjGnvwmq0h",
        "colab_type": "code",
        "outputId": "2f13f64a-f6ef-4fde-ecb4-87c0e7a041a9",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "print(corpus_lsidf)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "<gensim.interfaces.TransformedCorpus object at 0x7fe4aa473ac8>\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ldP2jzxsqHKB",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "simi_sorted = similarity(corpus_lsidf, query_lsidf)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "d_Tzb5ycjshY",
        "colab_type": "code",
        "outputId": "65a93edc-b9de-41de-b22d-30ed359c7e2f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "print(len(simi_sorted))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "8\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "outputId": "6024989d-1382-4d75-bdd9-b4563434517e",
        "id": "guvm0R8x36b7",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 121
        }
      },
      "source": [
        "    \n",
        "    print(\"simi_sorted \\n\")\n",
        "    print(simi_sorted)\n",
        "    candidate_ans = []\n",
        "    \n",
        "    ##New Correction\n",
        "    for sent in simi_sorted:\n",
        "        sent_id = sent[0]\n",
        "        if sent[1] > 0:\n",
        "          candidate_ans.append(str(sentences[sent_id]))\n",
        "          print(candidate_ans)\n",
        "        \n",
        "        \n",
        "        "
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "simi_sorted \n",
            "\n",
            "[(1, 0.7019072), (4, 0.2774532), (0, 0.20973489), (5, 0.0), (6, 0.0), (7, 0.0), (3, -2.3283064e-09), (2, -7.450581e-09)]\n",
            "['My age is 24.']\n",
            "['My age is 24.', 'My father name is Uttam Kumar gupta.']\n",
            "['My age is 24.', 'My father name is Uttam Kumar gupta.', 'My name is Sourav and I am from India.']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "97j0tI1iHKP3",
        "colab_type": "code",
        "outputId": "f258b76a-5948-49c7-966b-65a495ca05eb",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        }
      },
      "source": [
        " from gensim.corpora import Dictionary\n",
        "dct = Dictionary([\"máma mele maso\".split(), \"ema má máma\".split()])\n",
        "ch = dct.doc2bow([\"this\", \"is\", \"máma\", \"máma\"])\n",
        "print(dct)\n",
        "print(ch)\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Dictionary(5 unique tokens: ['maso', 'mele', 'máma', 'ema', 'má'])\n",
            "[(2, 2)]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "U53e1ivZfvm4",
        "colab_type": "code",
        "outputId": "5439d445-5db2-45d3-b4f5-546d0c1456a1",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "dct.doc2bow([\"this\", \"is\", \"máma\"], return_missing=True)\n",
        "([(2, 1)], {u'this': 1, u'is': 1})"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "([(2, 1)], {'is': 1, 'this': 1})"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 23
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5X_Xqu4V90w4",
        "colab_type": "text"
      },
      "source": [
        "### **dependency parse **"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "G1nukUOE97eL",
        "colab_type": "code",
        "outputId": "2b03a888-d237-409c-dba1-ccf422dd169a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "  print(candidate_ans)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['My age is 24.', 'My father name is Uttam Kumar gupta.', 'My name is Sourav and I am from India.']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NVzj7wyA-BLo",
        "colab_type": "code",
        "outputId": "6a41a4a1-4ed1-4b36-8c42-cdd10f85f4f7",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 416
        }
      },
      "source": [
        "doc = nlp('My age is 24. My father name is Uttam Kumar gupta. My name is Sourav and I am from India.')\n",
        " \n",
        "for token in doc:\n",
        "    print(\"{0}/{1} <--{2}-- {3}/{4}\".format(\n",
        "        token.text, token.tag_, token.dep_, token.head.text, token.head.tag_))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "My/PRP$ <--poss-- age/NN\n",
            "age/NN <--nsubj-- is/VBZ\n",
            "is/VBZ <--ROOT-- is/VBZ\n",
            "24/CD <--attr-- is/VBZ\n",
            "./. <--punct-- is/VBZ\n",
            "My/PRP$ <--poss-- name/NN\n",
            "father/NN <--compound-- name/NN\n",
            "name/NN <--nsubj-- is/VBZ\n",
            "is/VBZ <--ROOT-- is/VBZ\n",
            "Uttam/NNP <--compound-- Kumar/NNP\n",
            "Kumar/NNP <--compound-- gupta/NNP\n",
            "gupta/NNP <--attr-- is/VBZ\n",
            "./. <--punct-- is/VBZ\n",
            "My/PRP$ <--poss-- name/NN\n",
            "name/NN <--nsubj-- is/VBZ\n",
            "is/VBZ <--ROOT-- is/VBZ\n",
            "Sourav/NNP <--attr-- is/VBZ\n",
            "and/CC <--cc-- is/VBZ\n",
            "I/PRP <--nsubj-- am/VBP\n",
            "am/VBP <--conj-- is/VBZ\n",
            "from/IN <--prep-- am/VBP\n",
            "India/NNP <--pobj-- from/IN\n",
            "./. <--punct-- am/VBP\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3_bYtS05_Lga",
        "colab_type": "code",
        "outputId": "0dd9dd42-f3d4-4988-9239-30c60cc61716",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "spacy.explain(\"NNP\")"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'noun, proper singular'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 26
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZyEJpsVQkT39",
        "colab_type": "code",
        "outputId": "41290d1a-3303-4cfe-865e-731ac62c3a21",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 104
        }
      },
      "source": [
        "import spacy\n",
        "from spacy import displacy\n",
        " \n",
        "displacy.serve(doc, style=\"dep\")"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Using the 'dep' visualizer\n",
            "Serving on http://0.0.0.0:5000 ...\n",
            "\n",
            "Shutting down server on port 5000.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uRpLlVa7FRf5",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import numpy as np\n",
        "import re\n",
        "from nltk import word_tokenize, pos_tag\n",
        "from nltk.corpus import stopwords\n",
        "from collections import Counter\n",
        "\n",
        "\n",
        "#input text file\n",
        "readFlNm=\"input.txt\"\n",
        "#precentage of summarization\n",
        "percentageOfSummary=70\n",
        "#fetch stop words\n",
        "stopWords = set(stopwords.words('english'))\n",
        "#update stop words\n",
        "stopWords.update(['\"', \"'\", ':', '(', ')', '[', ']', '{', '}']) #'.',  ',', '?', '!', ';'\n",
        "\n",
        "#using pos_tag get the tag of words\n",
        "def getTagsForWords(textLn2):\n",
        "    tokens=word_tokenize(textLn2)\n",
        "    tagged=pos_tag(tokens)\n",
        "    return(tagged)\n",
        "\n",
        "#remove stop words\n",
        "def remStopWordsOur(lineIn):\n",
        "    stopWords= {'i','a','and','about','an','are','as','at','be','by','com','for','from','how','in','is','it','not','of','on','or','that','the','this','to','was','what','when','where','who','will','with','the','www','your','is','am','some','you','your','I','A','And','About','An','Are','As','At','Be','By','Com','For','From','How','In','Is','It','Not','Of','On','Or','That','The','This','To','Was','What','When','Where','Who','Will','With','The','Www','Your','Is','Am','Some','You','Your','Was'}\n",
        "    rmdStopWordsLn = ' '.join(w for w in lineIn.split() if w.lower() not in stopWords)\n",
        "    return rmdStopWordsLn\n",
        "\n",
        "#preprocessing the text and remove special characters\n",
        "def preprocessText(lineIn):\n",
        "    lineInLower=lineIn.lower()\n",
        "    lineInRmdSplChars=lineInLower.replace('.',' ').replace(';',' ').replace(',',' ').replace('?',' ').replace('!',' ').replace(':',' ')\n",
        "    return lineInRmdSplChars\n",
        "\n",
        "#Divide the given text into lines\n",
        "def getAllLines(lineIn):\n",
        "    lineInReplcByPeriod=lineIn.replace('.','.§').replace(';',';§').replace(',',',§').replace('?','?§').replace('!','!§').replace('\\n','§')\n",
        "    linesOriginal=lineInReplcByPeriod.split('§')\n",
        "    linesOriginal2=[item for item in linesOriginal if len(item)>0 ]\n",
        "    return linesOriginal2\n",
        "\n",
        "#Identify the noun position\n",
        "def getNounPositions(type,tagged):\n",
        "    nounPosi={}\n",
        "    for item in tagged:\n",
        "        if item[1]==type:\n",
        "            nounPosi[item[0]]=-1\n",
        "    \n",
        "    for key in nounPosi.keys():\n",
        "        regExpression=r'\\b'+key.lower()+r'\\b'\n",
        "        nounsi=[m.start() for m in re.finditer(regExpression, lineIn.lower())]\n",
        "#        print(key,nounsi)\n",
        "        nounPosi[key]=nounsi\n",
        "    return nounPosi\n",
        "#Identify the pronoun position\n",
        "def getProNounPositions(tagged):\n",
        "    proNounPosi={}\n",
        "    for item in tagged:\n",
        "        if item[1]=='PRP': #or item[1]=='PRP$':\n",
        "            proNounPosi[item[0].lower()]=-1\n",
        "    \n",
        "    for key in proNounPosi.keys():\n",
        "        regExpression=r'\\b'+key.lower()+r'\\b'\n",
        "        pronounsi=[m.start() for m in re.finditer(regExpression, lineIn.lower())]\n",
        "#        print(key,pronounsi)\n",
        "        proNounPosi[key]=pronounsi\n",
        "    return proNounPosi\n",
        "\n",
        "#Obtain nearest previous noun\n",
        "def getNearestPreviousNoun(NNP,posiOfPronoun):\n",
        "#    print('\\t',NNP)    \n",
        "    minimumDiff=len(lineIn)\n",
        "    nearKey=''\n",
        "    for keyNNP in NNP.keys():\n",
        "        for posNoun in NNP[keyNNP]:\n",
        "            if(posiOfPronoun>posNoun):\n",
        "#                print('\\t',posiOfPronoun-posNoun)\n",
        "                if(minimumDiff>(posiOfPronoun-posNoun)):\n",
        "                    minimumDiff=posiOfPronoun-posNoun\n",
        "                    nearKey=keyNNP\n",
        "#    print('\\t near key=',nearKey)\n",
        "    return nearKey\n",
        "\n",
        "#Replace pronoun by noun\n",
        "def pronounReplaceWithNearNoun(lineIn,PRP,NNP):\n",
        "    replacePRP=[]       \n",
        "    for key in PRP.keys():            \n",
        "        for pos in PRP[key]:\n",
        "            print('---------',key,'------',pos ,'-----')\n",
        "            nearNoun=getNearestPreviousNoun(NNP,pos)\n",
        "            replacePRP.append((key,pos,nearNoun))  \n",
        "#    print(PRP)\n",
        "#    print(replacePRP)\n",
        "    \n",
        "    replacePRP=sorted(replacePRP,key=lambda x:(-x[1],x[0],x[2]))\n",
        "    lineInReplacePronn=lineIn\n",
        "    for prpRep in replacePRP:\n",
        "        lineInReplacePronn=lineInReplacePronn[:prpRep[1]]+prpRep[2]+lineInReplacePronn[prpRep[1]+len(prpRep[0]):]\n",
        "    return lineInReplacePronn\n",
        "\n",
        "#Based on weightage obtain the priority of lines\n",
        "def obtainPriorotyOfALine(wtForLine):\n",
        "    orderdLinesByWt=np.argsort(wtForLine)\n",
        "    orderdLinesByWt=orderdLinesByWt[::-1]\n",
        "    priority=[0]*len(wtForLine)\n",
        "    \n",
        "    for i in range(len(wtForLine)):\n",
        "#        print(i,wtForLine[i],orderdLinesByWt[i])\n",
        "        priority[orderdLinesByWt[i]]=i\n",
        "    \n",
        "    sentWtAndPriority=[]\n",
        "    \n",
        "    for i in range(len(wtForLine)):\n",
        "        sentWtAndPriority.append((wtForLine[i],priority[i]))\n",
        "    \n",
        "    return sentWtAndPriority\n",
        "#Construct summary by extraction method\n",
        "def obtainSummary(lineForCalc,lineForExtract,percentageOfSummary):\n",
        "    wtForLine=[0]*len(lineForCalc)\n",
        "    print('Calcualting wt for lines......')\n",
        "    for li in range(len(lineForCalc)):\n",
        "    #    print('\\t'+linesOriginal2[li])\n",
        "        wtForLn=0.0\n",
        "        preproccdLn2=preprocessText(lineForCalc[li])\n",
        "        wInL=preproccdLn2.split()\n",
        "        for w in wInL:\n",
        "            w=preprocessText(w)\n",
        "            if w in list(freqOfWords.keys()):\n",
        "    #            print('\\t\\t'+w+' '+str(freqOfWords[w]))\n",
        "                wtForLn=wtForLn+freqOfWords[w]\n",
        "        wtForLine[li]=(wtForLn/len(wInL))\n",
        "    #        print('\\t\\t'+lineForCalc[li]+' $'+str(wtForLine[li]))\n",
        "    \n",
        "    sentWtAndPriority=obtainPriorotyOfALine(wtForLine)\n",
        "    print(sentWtAndPriority)\n",
        "    numOfLinesInSummary=int((percentageOfSummary*len(lineForCalc))/100)\n",
        "    reducedSummary=[]\n",
        "    for li in range(len(lineForExtract)):\n",
        "        if(sentWtAndPriority[li][1]<numOfLinesInSummary):\n",
        "    #            print(li,sentWtAndPriority[li])\n",
        "            reducedSummary.append(lineForExtract[li])\n",
        "    return reducedSummary\n",
        "\n",
        "#remove week nm and month name\n",
        "def removeWeekNmMonthNm(NNP):\n",
        "    entries = ('january','february','march','april','may','june','july','august','september','october','november','december','monday','tuesday','wednesday','thursday','friday','saturday','sunday')\n",
        "    delKeys=[]\n",
        "    for key in NNP.keys():\n",
        "        if key.lower() in entries:\n",
        "            print(key)\n",
        "            delKeys.append(key)\n",
        "    \n",
        "    for key in delKeys:\n",
        "        del NNP[key]\n",
        "    return NNP\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uXMWneDdMPWI",
        "colab_type": "code",
        "outputId": "1c331359-01fd-45fe-ae87-7fa4e21a0547",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 104
        }
      },
      "source": [
        "nltk.download('punkt')\n",
        "nltk.download('averaged_perceptron_tagger')\n",
        "from gensim.summarization.summarizer import summarize"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Unzipping taggers/averaged_perceptron_tagger.zip.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Lyq5KPt7QL8h",
        "colab_type": "code",
        "outputId": "084e8361-2469-4d39-9d3c-3e8c3e8b9b14",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 332
        }
      },
      "source": [
        "pip install gensim"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: gensim in /usr/local/lib/python3.6/dist-packages (3.6.0)\n",
            "Requirement already satisfied: six>=1.5.0 in /usr/local/lib/python3.6/dist-packages (from gensim) (1.12.0)\n",
            "Requirement already satisfied: scipy>=0.18.1 in /usr/local/lib/python3.6/dist-packages (from gensim) (1.3.1)\n",
            "Requirement already satisfied: numpy>=1.11.3 in /usr/local/lib/python3.6/dist-packages (from gensim) (1.16.5)\n",
            "Requirement already satisfied: smart-open>=1.2.1 in /usr/local/lib/python3.6/dist-packages (from gensim) (1.8.4)\n",
            "Requirement already satisfied: boto>=2.32 in /usr/local/lib/python3.6/dist-packages (from smart-open>=1.2.1->gensim) (2.49.0)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.6/dist-packages (from smart-open>=1.2.1->gensim) (2.21.0)\n",
            "Requirement already satisfied: boto3 in /usr/local/lib/python3.6/dist-packages (from smart-open>=1.2.1->gensim) (1.9.250)\n",
            "Requirement already satisfied: chardet<3.1.0,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests->smart-open>=1.2.1->gensim) (3.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests->smart-open>=1.2.1->gensim) (2019.9.11)\n",
            "Requirement already satisfied: idna<2.9,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests->smart-open>=1.2.1->gensim) (2.8)\n",
            "Requirement already satisfied: urllib3<1.25,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests->smart-open>=1.2.1->gensim) (1.24.3)\n",
            "Requirement already satisfied: botocore<1.13.0,>=1.12.250 in /usr/local/lib/python3.6/dist-packages (from boto3->smart-open>=1.2.1->gensim) (1.12.250)\n",
            "Requirement already satisfied: s3transfer<0.3.0,>=0.2.0 in /usr/local/lib/python3.6/dist-packages (from boto3->smart-open>=1.2.1->gensim) (0.2.1)\n",
            "Requirement already satisfied: jmespath<1.0.0,>=0.7.1 in /usr/local/lib/python3.6/dist-packages (from boto3->smart-open>=1.2.1->gensim) (0.9.4)\n",
            "Requirement already satisfied: docutils<0.16,>=0.10 in /usr/local/lib/python3.6/dist-packages (from botocore<1.13.0,>=1.12.250->boto3->smart-open>=1.2.1->gensim) (0.15.2)\n",
            "Requirement already satisfied: python-dateutil<3.0.0,>=2.1; python_version >= \"2.7\" in /usr/local/lib/python3.6/dist-packages (from botocore<1.13.0,>=1.12.250->boto3->smart-open>=1.2.1->gensim) (2.5.3)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ebsVe-5kQRot",
        "colab_type": "code",
        "outputId": "3c35a502-4a42-49d9-fdfb-07dbc6346290",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 87
        }
      },
      "source": [
        "\n",
        "lineIn = \"My name is Sourav and I am from India. My age is 24. I am currently stying in Passau. I am studing Mobile and embedded system. My father name is Uttam Kumar gupta. He is a business man. He own a shop. His work is of Photography. \"\n",
        "print( gensim.summarization.summarizer.summarize(lineIn, ratio=0.5, word_count=None, split=False))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "My name is Sourav and I am from India.\n",
            "My age is 24.\n",
            "I am studing Mobile and embedded system.\n",
            "He own a shop.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VUch04lCL3kf",
        "colab_type": "code",
        "outputId": "82b273ad-c877-4142-9be5-e1c2cf4d33c2",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 173
        }
      },
      "source": [
        "\n",
        "\n",
        "lineIn = \"My age is 24. My father name is Uttam Kumar gupta. My name is Sourav and I am from India.\"\n",
        "print( gensim.summarization.summarizer.summarize(lineIn,ratio=0.5, word_count=None, split=False))\n",
        "nt=len(lineIn.split())\n",
        "freqOfWords = Counter(re.split(r'\\s+',re.sub(r'[.,;\\-!?]','',lineIn)))\n",
        "for word, freq in freqOfWords.items():   \n",
        "  freqOfWords[word]=freqOfWords[word]/nt\n",
        "\n",
        "tagged=getTagsForWords(lineIn)\n",
        "NNP=getNounPositions('NNP',tagged)\n",
        "NNP=removeWeekNmMonthNm(NNP)\n",
        "#NN=getNounPositions('NN',tagged)\n",
        "#NNS=getNounPositions('NNS',tagged)\n",
        "PRP=getProNounPositions(tagged)\n",
        "\n",
        "linesOriginal2=getAllLines(lineIn)\n",
        "lineInReplacePronn=pronounReplaceWithNearNoun(lineIn,PRP,NNP)\n",
        "linesReplacedPronn2=getAllLines(lineInReplacePronn)\n",
        "print(linesReplacedPronn2)\n",
        "print(percentageOfSummary)\n",
        "\n",
        "#perform the text summarization without pronoun replacement\n",
        "reducedSummaryWithoutReplc=obtainSummary(linesOriginal2,linesOriginal2,percentageOfSummary)\n",
        "#perform the text summarization with pronoun replacement\n",
        "reducedSummaryWithReplc=obtainSummary(linesReplacedPronn2,linesOriginal2,percentageOfSummary)\n",
        "\n",
        "\n",
        "#text summeriazation.\n",
        "\n",
        "print(reducedSummaryWithReplc)\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "My age is 24.\n",
            "--------- i ------ 73 -----\n",
            "['My age is 24.', ' My father name is Uttam Kumar gupta.', ' My name is Sourav and Sourav am from India.']\n",
            "70\n",
            "Calcualting wt for lines......\n",
            "[(0.0625, 0), (0.05, 1), (0.04444444444444444, 2)]\n",
            "Calcualting wt for lines......\n",
            "[(0.0625, 0), (0.05, 1), (0.04444444444444444, 2)]\n",
            "['My age is 24.', ' My father name is Uttam Kumar gupta.']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "outputId": "18088fa4-db1f-45ac-cf52-d163e8a0ff96",
        "id": "Az8vX-psrEKR",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 505
        }
      },
      "source": [
        "doc = nlp(lineIn)\n",
        "\n",
        "from prettytable import PrettyTable\n",
        "t = PrettyTable(['text', 'token.head.text','token.head.tag_','child','pos_','tag_','dep_','shape_',])\n",
        " \n",
        "for token in doc:\n",
        "  child =  ([child for child in token.children])\n",
        "  t.add_row([token.text,token.head.text, spacy.explain(token.head.tag_), str(child), spacy.explain(token.pos_),spacy.explain(token.tag_) ,spacy.explain(token.dep_) ,\n",
        "          spacy.explain(token.shape_)])\n",
        "    \n",
        "print(t)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "+--------+-----------------+-------------------------------------------+-------------------------+--------------------------+-------------------------------------------+--------------------------+-----------------------------------+\n",
            "|  text  | token.head.text |              token.head.tag_              |          child          |           pos_           |                    tag_                   |           dep_           |               shape_              |\n",
            "+--------+-----------------+-------------------------------------------+-------------------------+--------------------------+-------------------------------------------+--------------------------+-----------------------------------+\n",
            "|   My   |       age       |           noun, singular or mass          |            []           |        determiner        |            pronoun, possessive            |   possession modifier    |                None               |\n",
            "|  age   |        is       |     verb, 3rd person singular present     |           [My]          |           noun           |           noun, singular or mass          |     nominal subject      |                None               |\n",
            "|   is   |        is       |     verb, 3rd person singular present     |       [age, 24, .]      |           verb           |     verb, 3rd person singular present     |           None           |                None               |\n",
            "|   24   |        is       |     verb, 3rd person singular present     |            []           |         numeral          |              cardinal number              |        attribute         |                None               |\n",
            "|   .    |        is       |     verb, 3rd person singular present     |            []           |       punctuation        |     punctuation mark, sentence closer     |       punctuation        | punctuation mark, sentence closer |\n",
            "|   My   |       name      |           noun, singular or mass          |            []           |        determiner        |            pronoun, possessive            |   possession modifier    |                None               |\n",
            "| father |       name      |           noun, singular or mass          |            []           |           noun           |           noun, singular or mass          |         compound         |                None               |\n",
            "|  name  |        is       |     verb, 3rd person singular present     |       [My, father]      |           noun           |           noun, singular or mass          |     nominal subject      |                None               |\n",
            "|   is   |        is       |     verb, 3rd person singular present     |     [name, gupta, .]    |           verb           |     verb, 3rd person singular present     |           None           |                None               |\n",
            "| Uttam  |      Kumar      |           noun, proper singular           |            []           |       proper noun        |           noun, proper singular           |         compound         |                None               |\n",
            "| Kumar  |      gupta      |           noun, proper singular           |         [Uttam]         |       proper noun        |           noun, proper singular           |         compound         |                None               |\n",
            "| gupta  |        is       |     verb, 3rd person singular present     |         [Kumar]         |       proper noun        |           noun, proper singular           |        attribute         |                None               |\n",
            "|   .    |        is       |     verb, 3rd person singular present     |            []           |       punctuation        |     punctuation mark, sentence closer     |       punctuation        | punctuation mark, sentence closer |\n",
            "|   My   |       name      |           noun, singular or mass          |            []           |        determiner        |            pronoun, possessive            |   possession modifier    |                None               |\n",
            "|  name  |        is       |     verb, 3rd person singular present     |           [My]          |           noun           |           noun, singular or mass          |     nominal subject      |                None               |\n",
            "|   is   |        is       |     verb, 3rd person singular present     | [name, Sourav, and, am] |           verb           |     verb, 3rd person singular present     |           None           |                None               |\n",
            "| Sourav |        is       |     verb, 3rd person singular present     |            []           |       proper noun        |           noun, proper singular           |        attribute         |                None               |\n",
            "|  and   |        is       |     verb, 3rd person singular present     |            []           | coordinating conjunction |         conjunction, coordinating         | coordinating conjunction |                None               |\n",
            "|   I    |        am       |   verb, non-3rd person singular present   |            []           |         pronoun          |             pronoun, personal             |     nominal subject      |               other               |\n",
            "|   am   |        is       |     verb, 3rd person singular present     |       [I, from, .]      |           verb           |   verb, non-3rd person singular present   |         conjunct         |                None               |\n",
            "|  from  |        am       |   verb, non-3rd person singular present   |         [India]         |        adposition        | conjunction, subordinating or preposition |  prepositional modifier  |                None               |\n",
            "| India  |       from      | conjunction, subordinating or preposition |            []           |       proper noun        |           noun, proper singular           |  object of preposition   |                None               |\n",
            "|   .    |        am       |   verb, non-3rd person singular present   |            []           |       punctuation        |     punctuation mark, sentence closer     |       punctuation        | punctuation mark, sentence closer |\n",
            "+--------+-----------------+-------------------------------------------+-------------------------+--------------------------+-------------------------------------------+--------------------------+-----------------------------------+\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "outputId": "97ab0d5e-3118-470f-cf4a-1ffcdd1fa5bb",
        "id": "xpfWKgnkqaXN",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 69
        }
      },
      "source": [
        "for ent in doc.ents:\n",
        "    print(ent.text, ent.start_char, ent.end_char, ent.label_)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "24 10 12 DATE\n",
            "Sourav 62 68 PERSON\n",
            "India 83 88 GPE\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bcJY3ybppfC2",
        "colab_type": "code",
        "outputId": "bad43316-1f02-4df6-fbe9-fe296a3df1af",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        }
      },
      "source": [
        "root = [token for token in doc if token.head == token][0]\n",
        "subject = list(root.lefts)[0]\n",
        "for descendant in subject.subtree:\n",
        "    assert subject is descendant or subject.is_ancestor(descendant)\n",
        "    print(descendant.text, descendant.dep_, descendant.n_lefts,\n",
        "            descendant.n_rights,\n",
        "            [ancestor.text for ancestor in descendant.ancestors])"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "My poss 0 0 ['age', 'is']\n",
            "age nsubj 1 0 ['is']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2rRVqe5RYhXI",
        "colab_type": "code",
        "outputId": "3fc89796-2508-41d5-d017-4357f9d0663b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 519
        }
      },
      "source": [
        "from spacy.pipeline import EntityLinker\n",
        "\n",
        "entity_linker = EntityLinker(nlp.vocab)\n",
        "entity_linker.from_disk(\"/path/to/model\")\n",
        "doc = nlp(\"This is a sentence.\")\n",
        "# This usually happens under the hood\n",
        "processed = entity_linker(doc)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-36-3494060da84d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mentity_linker\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mEntityLinker\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnlp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvocab\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mentity_linker\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_disk\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"/path/to/model\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0mdoc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnlp\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"This is a sentence.\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;31m# This usually happens under the hood\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32mpipes.pyx\u001b[0m in \u001b[0;36mspacy.pipeline.pipes.EntityLinker.from_disk\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/spacy/util.py\u001b[0m in \u001b[0;36mfrom_disk\u001b[0;34m(path, readers, exclude)\u001b[0m\n\u001b[1;32m    634\u001b[0m         \u001b[0;31m# Split to support file names like meta.json\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    635\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mkey\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\".\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mexclude\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 636\u001b[0;31m             \u001b[0mreader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    637\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mpath\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    638\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32mpipes.pyx\u001b[0m in \u001b[0;36mspacy.pipeline.pipes.EntityLinker.from_disk.lambda35\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;32mvocab.pyx\u001b[0m in \u001b[0;36mspacy.vocab.Vocab.from_disk\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;32mstrings.pyx\u001b[0m in \u001b[0;36mspacy.strings.StringStore.from_disk\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/srsly/_json_api.py\u001b[0m in \u001b[0;36mread_json\u001b[0;34m(location)\u001b[0m\n\u001b[1;32m     48\u001b[0m         \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstdin\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     49\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mujson\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloads\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 50\u001b[0;31m     \u001b[0mfile_path\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mforce_path\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlocation\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     51\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mfile_path\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"r\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mencoding\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"utf8\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     52\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mujson\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/srsly/util.py\u001b[0m in \u001b[0;36mforce_path\u001b[0;34m(location, require_exists)\u001b[0m\n\u001b[1;32m     19\u001b[0m         \u001b[0mlocation\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mPath\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlocation\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mrequire_exists\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mlocation\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexists\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 21\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Can't read file: {}\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlocation\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     22\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mlocation\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: Can't read file: /path/to/model/vocab/strings.json"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "prQFIGv9btBc",
        "colab_type": "code",
        "outputId": "9011fd92-ab9b-4cae-8678-b94f0b45cf2d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "doc = nlp('European authorities fined Google a record $5.1 billion on Wednesday for abusing its power in the mobile phone market and ordered the company to alter its practices')\n",
        "print([(X.text, X.label_) for X in doc.ents])"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[('European', 'NORP'), ('Google', 'ORG'), ('$5.1 billion', 'MONEY'), ('Wednesday', 'DATE')]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "M9mMEMvhgsI-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cdK63eRMww9D",
        "colab_type": "text"
      },
      "source": [
        "### **Sentiment**\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "otUO23Nmw0zP",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from nltk.classify import NaiveBayesClassifier\n",
        "from nltk.corpus import subjectivity\n",
        "from nltk.sentiment import SentimentAnalyzer\n",
        "from nltk.sentiment.util import *\n",
        "from nltk.sentiment.vader import SentimentIntensityAnalyzer"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "slsETCzyxoLk",
        "colab_type": "code",
        "outputId": "24ef304c-9ed9-4a44-daf0-02b5debd080c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        }
      },
      "source": [
        "nltk.download('vader_lexicon')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package vader_lexicon to /root/nltk_data...\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 42
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wt4TRaO2w871",
        "colab_type": "code",
        "outputId": "a3350eea-d0f6-4561-f82a-ff56c92acb63",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 780
        }
      },
      "source": [
        " tricky_sentences = [\n",
        "    \"Most automated sentiment analysis tools are shit.\",\n",
        "    \"VADER sentiment analysis is the shit.\",\n",
        "    \"Sentiment analysis has never been good.\",\n",
        "    \"Sentiment analysis with VADER has never been this good.\",\n",
        "    \"Warren Beatty has never been so entertaining.\",\n",
        "    \"I won't say that the movie is astounding and I wouldn't claim that \\\n",
        "    the movie is too banal either.\",\n",
        "    \"I like to hate Michael Bay films, but I couldn't fault this one\",\n",
        "    \"It's one thing to watch an Uwe Boll film, but another thing entirely \\\n",
        "    to pay for it\",\n",
        "    \"The movie was too good\",\n",
        "    \"This movie was actually neither that funny, nor super witty.\",\n",
        "    \"This movie doesn't care about cleverness, wit or any other kind of \\\n",
        "    intelligent humor.\",\n",
        "    \"Those who find ugly meanings in beautiful things are corrupt without \\\n",
        "    being charming.\",\n",
        "    \"There are slow and repetitive parts, BUT it has just enough spice to \\\n",
        "    keep it interesting.\",\n",
        "    \"The script is not fantastic, but the acting is decent and the cinematography \\\n",
        "    is EXCELLENT!\",\n",
        "    \"Roger Dodger is one of the most compelling variations on this theme.\",\n",
        "    \"Roger Dodger is one of the least compelling variations on this theme.\",\n",
        "    \"Roger Dodger is at least compelling as a variation on the theme.\",\n",
        "    \"they fall in love with the product\",\n",
        "    \"but then it breaks\",\n",
        "    \"usually around the time the 90 day warranty expires\",\n",
        "    \"this is  excellent\",\n",
        "    \"However, Mr. Carter solemnly argues, his client carried out the kidnapping \\\n",
        "    under orders and in the ''least offensive way possible.''\"\n",
        " ]\n",
        " sentences.extend(tricky_sentences)\n",
        " sid = SentimentIntensityAnalyzer()\n",
        " for sentence in tricky_sentences :\n",
        "     print(sentence)\n",
        "     ss = sid.polarity_scores(sentence)\n",
        "     for k in sorted(ss):\n",
        "         print('{0}: {1}, '.format(k, ss[k]), end='')\n",
        "     print()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Most automated sentiment analysis tools are shit.\n",
            "compound: -0.5574, neg: 0.375, neu: 0.625, pos: 0.0, \n",
            "VADER sentiment analysis is the shit.\n",
            "compound: 0.6124, neg: 0.0, neu: 0.556, pos: 0.444, \n",
            "Sentiment analysis has never been good.\n",
            "compound: -0.3412, neg: 0.325, neu: 0.675, pos: 0.0, \n",
            "Sentiment analysis with VADER has never been this good.\n",
            "compound: 0.5228, neg: 0.0, neu: 0.703, pos: 0.297, \n",
            "Warren Beatty has never been so entertaining.\n",
            "compound: 0.5777, neg: 0.0, neu: 0.616, pos: 0.384, \n",
            "I won't say that the movie is astounding and I wouldn't claim that    the movie is too banal either.\n",
            "compound: 0.4215, neg: 0.0, neu: 0.851, pos: 0.149, \n",
            "I like to hate Michael Bay films, but I couldn't fault this one\n",
            "compound: 0.3153, neg: 0.157, neu: 0.534, pos: 0.309, \n",
            "It's one thing to watch an Uwe Boll film, but another thing entirely    to pay for it\n",
            "compound: -0.2541, neg: 0.112, neu: 0.888, pos: 0.0, \n",
            "The movie was too good\n",
            "compound: 0.4404, neg: 0.0, neu: 0.58, pos: 0.42, \n",
            "This movie was actually neither that funny, nor super witty.\n",
            "compound: -0.6759, neg: 0.41, neu: 0.59, pos: 0.0, \n",
            "This movie doesn't care about cleverness, wit or any other kind of    intelligent humor.\n",
            "compound: -0.1338, neg: 0.265, neu: 0.497, pos: 0.239, \n",
            "Those who find ugly meanings in beautiful things are corrupt without    being charming.\n",
            "compound: -0.3553, neg: 0.314, neu: 0.493, pos: 0.192, \n",
            "There are slow and repetitive parts, BUT it has just enough spice to    keep it interesting.\n",
            "compound: 0.4678, neg: 0.079, neu: 0.735, pos: 0.186, \n",
            "The script is not fantastic, but the acting is decent and the cinematography    is EXCELLENT!\n",
            "compound: 0.7565, neg: 0.092, neu: 0.607, pos: 0.301, \n",
            "Roger Dodger is one of the most compelling variations on this theme.\n",
            "compound: 0.2944, neg: 0.0, neu: 0.834, pos: 0.166, \n",
            "Roger Dodger is one of the least compelling variations on this theme.\n",
            "compound: -0.1695, neg: 0.132, neu: 0.868, pos: 0.0, \n",
            "Roger Dodger is at least compelling as a variation on the theme.\n",
            "compound: 0.2263, neg: 0.0, neu: 0.84, pos: 0.16, \n",
            "they fall in love with the product\n",
            "compound: 0.6369, neg: 0.0, neu: 0.588, pos: 0.412, \n",
            "but then it breaks\n",
            "compound: 0.0, neg: 0.0, neu: 1.0, pos: 0.0, \n",
            "usually around the time the 90 day warranty expires\n",
            "compound: 0.0, neg: 0.0, neu: 1.0, pos: 0.0, \n",
            "this is  excellent\n",
            "compound: 0.5719, neg: 0.0, neu: 0.351, pos: 0.649, \n",
            "However, Mr. Carter solemnly argues, his client carried out the kidnapping    under orders and in the ''least offensive way possible.''\n",
            "compound: -0.5859, neg: 0.23, neu: 0.697, pos: 0.074, \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "V8x_2kFgxZjl",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}