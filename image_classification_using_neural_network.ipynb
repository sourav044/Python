{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "image classification using neural network.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 2",
      "name": "python2"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/sourav044/Python/blob/master/image_classification_using_neural_network.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "metadata": {
        "colab_type": "text",
        "id": "YHK6DyunSbs4"
      },
      "cell_type": "markdown",
      "source": [
        "# Cat vs. Dog Image Classification\n",
        "\n",
        "Tasks:-\n",
        "\n",
        "\n",
        "\n",
        "```\n",
        "# We will build a keras model to predict dogs and cats.  \n",
        "#We will be using a pretrained model ResNet18 to predict dogs and cats.\n",
        "```\n",
        "\n"
      ]
    },
    {
      "metadata": {
        "id": "iEAkj_71rS_R",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "#Training and Testing Model \n",
        "\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "Steps : \n",
        "\n",
        "\n",
        "```\n",
        "# Downloading the files \n",
        "# Extracting the files\n",
        "# Training \n",
        "# Testing \n",
        "\n",
        "\n",
        "```\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "metadata": {
        "colab_type": "text",
        "id": "UY6KJV6z6l7_"
      },
      "cell_type": "markdown",
      "source": [
        "## Explore Dataset"
      ]
    },
    {
      "metadata": {
        "id": "zL6vBgbQws71",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import os"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "RXZT2UsyIVe_",
        "outputId": "2b2c28cc-3e30-4c1f-da01-9f067cfff7b5",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        }
      },
      "cell_type": "code",
      "source": [
        "!wget http://files.fast.ai/data/dogscats.zip"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "--2019-04-28 09:09:03--  http://files.fast.ai/data/dogscats.zip\n",
            "Resolving files.fast.ai (files.fast.ai)... 67.205.15.147\n",
            "Connecting to files.fast.ai (files.fast.ai)|67.205.15.147|:80... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 857214334 (818M) [application/zip]\n",
            "Saving to: ‘dogscats.zip’\n",
            "\n",
            "dogscats.zip        100%[===================>] 817.50M  40.4MB/s    in 21s     \n",
            "\n",
            "2019-04-28 09:09:24 (39.5 MB/s) - ‘dogscats.zip’ saved [857214334/857214334]\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "PLy3pthUS0D2",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "!unzip dogscats.zip"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "colab_type": "text",
        "id": "o-qUPyfO7Qr8"
      },
      "cell_type": "markdown",
      "source": [
        "The contents of the .zip are extracted to the base directory `/content/dogscats/`, which contains `train` and `valid` subdirectories for the training."
      ]
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "MLZKVtE0dSfk",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "train_dir = 'dogscats/train'\n",
        "validation_dir = 'dogscats/valid'\n",
        "\n",
        "# Directory with our training cat pictures\n",
        "train_cats_dir = os.path.join(train_dir, 'cats')\n",
        "\n",
        "# Directory with our training dog pictures\n",
        "train_dogs_dir = os.path.join(train_dir, 'dogs')\n",
        "\n",
        "# Directory with our validation cat pictures\n",
        "validation_cats_dir = os.path.join(validation_dir, 'cats')\n",
        "\n",
        "# Directory with our validation dog pictures\n",
        "validation_dogs_dir = os.path.join(validation_dir, 'dogs')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "colab_type": "text",
        "id": "LuBYtA_Zd8_T"
      },
      "cell_type": "markdown",
      "source": [
        "Now, let's see what the filenames look like in the `cats` and `dogs` `train` directories (file naming conventions are the same in the `validation` directory):"
      ]
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "4PIP1rkmeAYS",
        "outputId": "375cd567-25d8-4eab-c507-a746cb3eff46",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 71
        }
      },
      "cell_type": "code",
      "source": [
        "train_cat_fnames = os.listdir(train_cats_dir)\n",
        "\n",
        "print train_cat_fnames[:10]\n",
        "\n",
        "train_dog_fnames = os.listdir(train_dogs_dir)\n",
        "train_dog_fnames.sort()\n",
        "print train_dog_fnames[:10]"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['cat.6273.jpg', 'cat.8338.jpg', 'cat.4824.jpg', 'cat.11815.jpg', 'cat.4891.jpg', 'cat.10044.jpg', 'cat.4273.jpg', 'cat.10549.jpg', 'cat.5761.jpg', 'cat.10789.jpg']\n",
            "['dog.0.jpg', 'dog.1.jpg', 'dog.10.jpg', 'dog.100.jpg', 'dog.1000.jpg', 'dog.10000.jpg', 'dog.10002.jpg', 'dog.10003.jpg', 'dog.10004.jpg', 'dog.10006.jpg']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "colab_type": "text",
        "id": "HlqN5KbafhLI"
      },
      "cell_type": "markdown",
      "source": [
        "Let's find out the total number of cat and dog images in the `train` and `validation` directories:"
      ]
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "H4XHh2xSfgie",
        "outputId": "391269bb-06e9-47c1-b9d6-e6146cd4b0a7",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 85
        }
      },
      "cell_type": "code",
      "source": [
        "print 'total training cat images:', len(os.listdir(train_cats_dir))\n",
        "print 'total training dog images:', len(os.listdir(train_dogs_dir))\n",
        "print 'total validation cat images:', len(os.listdir(validation_cats_dir))\n",
        "print 'total validation dog images:', len(os.listdir(validation_dogs_dir))"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "total training cat images: 11500\n",
            "total training dog images: 11500\n",
            "total validation cat images: 1000\n",
            "total validation dog images: 1000\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "colab_type": "text",
        "id": "C3WZABE9eX-8"
      },
      "cell_type": "markdown",
      "source": [
        "For both cats and dogs, we have 11500 training images and 1000 test images.\n"
      ]
    },
    {
      "metadata": {
        "colab_type": "text",
        "id": "5oqBkNBJmtUv"
      },
      "cell_type": "markdown",
      "source": [
        "## Processing Model  - Training and Testing using Neural Network\n",
        "\n",
        "The images that will go into our convnet are 150x150 color images (in the next section on Data Preprocessing, we'll add handling to resize all the images to 150x150 before feeding them into the neural network).\n",
        "\n",
        "Let's code up the architecture. \n",
        "We will stack 3 {convolution + relu + maxpooling} modules. \n",
        "\n",
        "Our convolutions operate on 3x3 windows and our maxpooling layers operate on 2x2 windows. \n",
        "Our first convolution extracts 16 filters, the following one extracts 32 filters, and the last one extracts 64 filters with 128 filter."
      ]
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "IYVLSfbsbf9K",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "from keras.models import Sequential\n",
        "from keras.layers import Conv2D, MaxPooling2D\n",
        "from keras.layers import Activation, Dropout, Flatten, Dense"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "dqqNIBnybjpa",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "model = Sequential()\n",
        "img_width, img_height = 150,150\n",
        "#150x150 for the image pixels, and 3 for the three color channels: R, G, and B\n",
        "img_input = (img_width, img_height, 3)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "ugysGIPxnaYo",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# First convolution extracts 16 filters that are 3x3\n",
        "# Convolution is followed by max-pooling layer with a 2x2 window\n",
        "model.add(Conv2D(16, (3, 3), input_shape=img_input))\n",
        "model.add(Activation('relu'))\n",
        "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
        "\n",
        "model.add(Conv2D(32, (3, 3), input_shape=img_input))\n",
        "model.add(Activation('relu'))\n",
        "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
        "\n",
        "model.add(Conv2D(64, (3, 3), input_shape=img_input))\n",
        "model.add(Activation('relu'))\n",
        "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
        "\n",
        "# Fourth convolution extracts 128 filters that are 3x3\n",
        "# Convolution is followed by max-pooling layer with a 2x2 window\n",
        "#x = layers.Conv2D(128, 3, activation='relu')(x)\n",
        "#x = layers.MaxPooling2D(2)(x)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "3v88_ZTAslvR",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# Flatten feature map to a 1-dim tensor so we can add fully connected layers\n",
        "model.add(Flatten())\n",
        "\n",
        "# ReLU activation and 512 hidden units\n",
        "model.add(Dense(512))\n",
        "model.add(Activation('relu'))\n",
        "\n",
        "\n",
        "# Add a dropout rate of 0.5\n",
        "model.add(Dropout(0.5))\n",
        "model.add(Dense(1))\n",
        "model.add(Activation('sigmoid'))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "colab_type": "text",
        "id": "s9EaFDP5srBa"
      },
      "cell_type": "markdown",
      "source": [
        "Let's summarize the model architecture:"
      ]
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "7ZKj8392nbgP",
        "outputId": "2c6db2b2-b86d-4f37-a8d0-0c2b9f68cd48",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 646
        }
      },
      "cell_type": "code",
      "source": [
        "model.summary()"
      ],
      "execution_count": 47,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "conv2d_3 (Conv2D)            (None, 148, 148, 16)      448       \n",
            "_________________________________________________________________\n",
            "activation_5 (Activation)    (None, 148, 148, 16)      0         \n",
            "_________________________________________________________________\n",
            "max_pooling2d_1 (MaxPooling2 (None, 74, 74, 16)        0         \n",
            "_________________________________________________________________\n",
            "conv2d_4 (Conv2D)            (None, 72, 72, 32)        4640      \n",
            "_________________________________________________________________\n",
            "activation_6 (Activation)    (None, 72, 72, 32)        0         \n",
            "_________________________________________________________________\n",
            "max_pooling2d_2 (MaxPooling2 (None, 36, 36, 32)        0         \n",
            "_________________________________________________________________\n",
            "conv2d_5 (Conv2D)            (None, 34, 34, 64)        18496     \n",
            "_________________________________________________________________\n",
            "activation_7 (Activation)    (None, 34, 34, 64)        0         \n",
            "_________________________________________________________________\n",
            "max_pooling2d_3 (MaxPooling2 (None, 17, 17, 64)        0         \n",
            "_________________________________________________________________\n",
            "flatten_3 (Flatten)          (None, 18496)             0         \n",
            "_________________________________________________________________\n",
            "dense_5 (Dense)              (None, 512)               9470464   \n",
            "_________________________________________________________________\n",
            "activation_8 (Activation)    (None, 512)               0         \n",
            "_________________________________________________________________\n",
            "dropout_3 (Dropout)          (None, 512)               0         \n",
            "_________________________________________________________________\n",
            "dense_6 (Dense)              (None, 1)                 513       \n",
            "_________________________________________________________________\n",
            "activation_9 (Activation)    (None, 1)                 0         \n",
            "=================================================================\n",
            "Total params: 9,494,561\n",
            "Trainable params: 9,494,561\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "HKHtBY08-NAw",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Model \n",
        "\n",
        "\n",
        "```\n",
        "We have used keras neural network to create our model and the model has \n",
        "```\n",
        "\n"
      ]
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "8DHWhFP_uhq3",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.optimizers import RMSprop\n",
        "\n",
        "model.compile(loss='binary_crossentropy',\n",
        "              optimizer='adam',\n",
        "              metrics=['acc'])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "ClebU9NJg99G",
        "outputId": "b95898b4-56bb-4c72-c468-eed56b26a453",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "\n",
        "batch_size_value = 20\n",
        "\n",
        "\n",
        "# Adding rescale, rotation_range, width_shift_range, height_shift_range,\n",
        "# shear_range, zoom_range, and horizontal flip to our ImageDataGenerator\n",
        "train_datagen = ImageDataGenerator(\n",
        "    rescale=1./255,\n",
        "    rotation_range=40,\n",
        "    width_shift_range=0.2,\n",
        "    height_shift_range=0.2,\n",
        "    shear_range=0.2,\n",
        "    zoom_range=0.2,\n",
        "    horizontal_flip=True,)\n",
        "\n",
        "# All images will be rescaled by 1./255\n",
        "test_datagen = ImageDataGenerator(rescale=1./255)\n",
        "\n",
        "# Flow training images in batches of 20 using train_datagen generator\n",
        "train_generator = train_datagen.flow_from_directory(\n",
        "        train_dir,  # This is the source directory for training images\n",
        "        target_size=(150, 150),  # All images will be resized to 150x150\n",
        "        batch_size=batch_size_value,\n",
        "        # Since we use binary_crossentropy loss, we need binary labels\n",
        "        class_mode='binary')\n",
        "\n",
        "# Flow validation images in batches of 20 using test_datagen generator\n",
        "validation_generator = test_datagen.flow_from_directory(\n",
        "        validation_dir,\n",
        "        target_size=(150, 150),\n",
        "        batch_size=batch_size_value,\n",
        "        class_mode='binary')\n",
        "\n",
        "# class_mode: one of \"categorical\", \"binary\", \"sparse\", \"input\", \"other\" or None. Default: \"categorical\". Mode for yielding the targets:\n",
        "#\"binary\": 1D numpy array of binary labels,\n",
        "#\"categorical\": 2D numpy array of one-hot encoded labels. Supports multi-label output."
      ],
      "execution_count": 49,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Found 23000 images belonging to 2 classes.\n",
            "Found 2000 images belonging to 2 classes.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "Fb1_lgobv81m",
        "outputId": "1b9be407-7630-4e9a-df41-143db35cd1aa",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1054
        }
      },
      "cell_type": "code",
      "source": [
        "model.fit_generator(\n",
        "      train_generator,\n",
        "      steps_per_epoch=23000 // batch_size_value,  # 2000 images = batch_size * steps\n",
        "      epochs=30,\n",
        "      validation_data=validation_generator,\n",
        "      validation_steps=2000 // batch_size_value,  # 1000 images = batch_size * steps\n",
        "      verbose=2)\n",
        "\n",
        "#Save the model.....\n",
        "#model.save_weights('first_try.h5')"
      ],
      "execution_count": 51,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/30\n",
            " - 184s - loss: 0.6232 - acc: 0.6543 - val_loss: 0.5322 - val_acc: 0.7305\n",
            "Epoch 2/30\n",
            " - 185s - loss: 0.5746 - acc: 0.6999 - val_loss: 0.5190 - val_acc: 0.7425\n",
            "Epoch 3/30\n",
            " - 182s - loss: 0.5528 - acc: 0.7175 - val_loss: 0.4461 - val_acc: 0.8030\n",
            "Epoch 4/30\n",
            " - 184s - loss: 0.5223 - acc: 0.7439 - val_loss: 0.4905 - val_acc: 0.7710\n",
            "Epoch 5/30\n",
            " - 183s - loss: 0.5156 - acc: 0.7483 - val_loss: 0.4592 - val_acc: 0.7745\n",
            "Epoch 6/30\n",
            " - 180s - loss: 0.5024 - acc: 0.7563 - val_loss: 0.4652 - val_acc: 0.7775\n",
            "Epoch 7/30\n",
            " - 183s - loss: 0.4821 - acc: 0.7677 - val_loss: 0.4174 - val_acc: 0.8055\n",
            "Epoch 8/30\n",
            " - 182s - loss: 0.4701 - acc: 0.7785 - val_loss: 0.4106 - val_acc: 0.8215\n",
            "Epoch 9/30\n",
            " - 186s - loss: 0.4599 - acc: 0.7860 - val_loss: 0.4092 - val_acc: 0.8145\n",
            "Epoch 10/30\n",
            " - 184s - loss: 0.4480 - acc: 0.7963 - val_loss: 0.3995 - val_acc: 0.8250\n",
            "Epoch 11/30\n",
            " - 181s - loss: 0.4328 - acc: 0.8024 - val_loss: 0.3815 - val_acc: 0.8325\n",
            "Epoch 12/30\n",
            " - 181s - loss: 0.4307 - acc: 0.8038 - val_loss: 0.3687 - val_acc: 0.8455\n",
            "Epoch 13/30\n",
            " - 181s - loss: 0.4162 - acc: 0.8137 - val_loss: 0.3527 - val_acc: 0.8465\n",
            "Epoch 14/30\n",
            " - 180s - loss: 0.4045 - acc: 0.8173 - val_loss: 0.3915 - val_acc: 0.8295\n",
            "Epoch 15/30\n",
            " - 178s - loss: 0.4085 - acc: 0.8163 - val_loss: 0.3902 - val_acc: 0.8170\n",
            "Epoch 16/30\n",
            " - 179s - loss: 0.3998 - acc: 0.8214 - val_loss: 0.3408 - val_acc: 0.8555\n",
            "Epoch 17/30\n",
            " - 182s - loss: 0.3871 - acc: 0.8283 - val_loss: 0.3506 - val_acc: 0.8530\n",
            "Epoch 18/30\n",
            " - 178s - loss: 0.3852 - acc: 0.8277 - val_loss: 0.4471 - val_acc: 0.7920\n",
            "Epoch 19/30\n",
            " - 181s - loss: 0.3847 - acc: 0.8290 - val_loss: 0.3326 - val_acc: 0.8530\n",
            "Epoch 20/30\n",
            " - 177s - loss: 0.3879 - acc: 0.8281 - val_loss: 0.3618 - val_acc: 0.8370\n",
            "Epoch 21/30\n",
            " - 180s - loss: 0.3727 - acc: 0.8372 - val_loss: 0.3379 - val_acc: 0.8445\n",
            "Epoch 22/30\n",
            " - 179s - loss: 0.3690 - acc: 0.8361 - val_loss: 0.3831 - val_acc: 0.8290\n",
            "Epoch 23/30\n",
            " - 181s - loss: 0.3686 - acc: 0.8374 - val_loss: 0.3350 - val_acc: 0.8470\n",
            "Epoch 24/30\n",
            " - 180s - loss: 0.3609 - acc: 0.8430 - val_loss: 0.3052 - val_acc: 0.8615\n",
            "Epoch 25/30\n",
            " - 189s - loss: 0.3614 - acc: 0.8420 - val_loss: 0.3366 - val_acc: 0.8500\n",
            "Epoch 26/30\n",
            " - 189s - loss: 0.3648 - acc: 0.8420 - val_loss: 0.3423 - val_acc: 0.8450\n",
            "Epoch 27/30\n",
            " - 185s - loss: 0.3514 - acc: 0.8443 - val_loss: 0.3332 - val_acc: 0.8640\n",
            "Epoch 28/30\n",
            " - 184s - loss: 0.3542 - acc: 0.8451 - val_loss: 0.3669 - val_acc: 0.8565\n",
            "Epoch 29/30\n",
            " - 186s - loss: 0.3529 - acc: 0.8473 - val_loss: 0.3204 - val_acc: 0.8625\n",
            "Epoch 30/30\n",
            " - 183s - loss: 0.3494 - acc: 0.8486 - val_loss: 0.3285 - val_acc: 0.8555\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7f2e5419fd10>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 51
        }
      ]
    },
    {
      "metadata": {
        "id": "NY4lHW8t0tPV",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "#Testing"
      ]
    },
    {
      "metadata": {
        "id": "XLEq5xQd7dDS",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "On top of it we stick two fully-connected layers. Because we are facing a two-class classification problem, i.e. a *binary classification problem*, we will end our network with a [*sigmoid* activation](https://wikipedia.org/wiki/Sigmoid_function), so that the output of our network will be a single scalar between 0 and 1, encoding the probability that the current image is class 1 (as opposed to class 0)."
      ]
    },
    {
      "metadata": {
        "id": "dZBN1STY1ao8",
        "colab_type": "code",
        "outputId": "c791c3c3-b4c5-461a-ea71-3c53842de4d1",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import random\n",
        "from tensorflow.keras.preprocessing.image import img_to_array, load_img\n",
        "\n",
        "# Let's prepare a random input image of a cat or dog from the training set.\n",
        "cat_img_files = os.path.join(validation_dir,'dogs') + '/dog.10001.jpg'\n",
        "img_path = cat_img_files\n",
        "\n",
        "img = load_img(img_path, target_size=(150, 150))  # this is a PIL image\n",
        "x = img_to_array(img)  # Numpy array with shape (150, 150, 3)\n",
        "x = x.reshape((1,) + x.shape)  # Numpy array with shape (1, 150, 150, 3)\n",
        "\n",
        "\n",
        "\n",
        "binary_val = model.predict(x, batch_size=20, verbose=0)\n",
        "print(binary_val)"
      ],
      "execution_count": 53,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[1.]]\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}